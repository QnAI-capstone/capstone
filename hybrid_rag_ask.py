from config import OPENAI_API_KEY
import openai
import tiktoken
import numpy as np
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from chromadb import PersistentClient
from chromadb.utils.embedding_functions import EmbeddingFunction
import re # ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©ì„ ìœ„í•´ ì¶”ê°€
from rapidfuzz import process
from dictionary import ABBREVIATION_GROUPS

# âœ… API í‚¤
openai.api_key = OPENAI_API_KEY

# âœ… í•œêµ­ì–´ ì„ë² ë”© í•¨ìˆ˜
class KoSimCSEEmbedding(EmbeddingFunction):
    def __init__(self):
        self.model = SentenceTransformer("jhgan/ko-sbert-sts")
    def __call__(self, input):
        return self.model.encode(input).tolist()

# âœ… ChromaDB ì´ˆê¸°í™” ë° ë¬¸ì„œ+ë©”íƒ€ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
def load_corpus():
    client = PersistentClient(path="./chroma_store")
    embedding_fn = KoSimCSEEmbedding()
    collections_info = client.list_collections() # get_collection ëŒ€ì‹  list_collections ì‚¬ìš©
    print(f"ì´ {len(collections_info)}ê°œì˜ ì»¬ë ‰ì…˜ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")

    corpus, metadatas_list = [], []
    unique_majors = set() # ê³ ìœ  í•™ê³¼ëª… ì €ì¥ì„ ìœ„í•œ set

    for col_info in collections_info:
        collection = client.get_collection(name=col_info.name, embedding_function=embedding_fn)
        data = collection.get(include=["documents", "metadatas"])
        corpus.extend(data["documents"])
        metadatas_list.extend(data["metadatas"])
        for meta in data["metadatas"]:
            if meta and 'major' in meta: # ë©”íƒ€ë°ì´í„° ë° 'major' í‚¤ ì¡´ì¬ í™•ì¸
                unique_majors.add(meta['major'])

    return corpus, metadatas_list, list(unique_majors) # ê³ ìœ  í•™ê³¼ëª… ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

def count_total_tokens(messages, model="gpt-4o"):
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")

    total_tokens = 0
    for message in messages:
        total_tokens += 4
        for key, value in message.items():
            total_tokens += len(encoding.encode(value))
    total_tokens += 2
    return total_tokens

# âœ… ì‚¬ìš©ì ì§ˆì˜ì—ì„œ í•™ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_major_keyword(query, majors_list):
    """
    ì‚¬ìš©ì ì§ˆì˜ì—ì„œ ì–¸ê¸‰ëœ í•™ê³¼ í‚¤ì›Œë“œë¥¼ majors_list (DBì— ì €ì¥ëœ ì‹¤ì œ í•™ê³¼ëª…) ê¸°ì¤€ìœ¼ë¡œ ìœ ì‚¬ ë¬¸ìì—´ ë§¤ì¹­í•˜ì—¬ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë„ì–´ì“°ê¸°, ì˜¤íƒ€, ì¶•ì•½ì–´ ì°¨ì´ ë“±ìœ¼ë¡œ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì•„ë„ ê°€ì¥ ìœ ì‚¬í•œ í•™ê³¼ëª…ì„ ì°¾ì•„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ì‚¬ìš©ì ì§ˆì˜ ì •ê·œí™”: ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜
    normalized_query = query.replace(" ", "").lower()

    # majors_listì˜ í•™ê³¼ëª…ë„ ì •ê·œí™” (ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°, ì†Œë¬¸ì ë³€í™˜)
    candidates = [m.replace("_", "").lower() for m in majors_list]

    # rapidfuzzì˜ extractOneìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ í•™ê³¼ëª…ê³¼ ìœ ì‚¬ë„ ë°˜í™˜
    best_match = process.extractOne(normalized_query, candidates)

    # ìœ ì‚¬ë„ ê¸°ì¤€ ì„¤ì • (ì˜ˆ: 80 ì´ìƒì¼ ë•Œë§Œ ë§¤ì¹­ ì¸ì •)
    if best_match and best_match[1] > 80:
        idx = candidates.index(best_match[0])
        return majors_list[idx]


    # "ê³µì§€", "ì•ˆë‚´", "ì¼ì •" ë“± ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë„ ì—¬ê¸°ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì˜ˆë¥¼ ë“¤ì–´ 'ê³µì§€'ë¼ëŠ” ë‹¨ì–´ê°€ ìˆìœ¼ë©´, ë¬¸ì„œ ìœ í˜•ì„ 'ê³µì§€'ë¡œ í•„í„°ë§í•˜ë„ë¡ ì„¤ì •
    # if "ê³µì§€" in query:
    # return {"type": "notice"} # ì´ëŸ° ì‹ìœ¼ë¡œ ë‹¤ë¥¸ í•„í„°ë§ ê¸°ì¤€ë„ ì¶”ê°€ ê°€ëŠ¥

    # ë§¤ì¹­ë˜ëŠ” í•™ê³¼ëª…ì´ ì—†ìœ¼ë©´ None ë°˜í™˜
    return None


# âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
class HybridRetriever:
    def __init__(self, corpus_all, metadatas_all):
        # ì´ˆê¸°í™” ì‹œì ì—ëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ë³´ê´€
        self.corpus_all = corpus_all
        self.metadatas_all = metadatas_all
        self.encoder = SentenceTransformer("jhgan/ko-sbert-sts")
        # ì „ì²´ ë¬¸ì„œì— ëŒ€í•œ ì„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°í•´ë‘˜ ìˆ˜ ìˆìœ¼ë‚˜, í•„í„°ë§ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³ ë ¤
        # self.dense_embeddings_all = self.encoder.encode(self.corpus_all) # í•„ìš” ì‹œ í™œì„±í™”

    def retrieve(self, query, top_k_bm25=10, top_k_dpr=3, filter_major=None):
        # ì‹¤ì œ ê²€ìƒ‰ ëŒ€ìƒì´ ë  ì½”í¼ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°
        current_corpus = self.corpus_all
        current_metadatas = self.metadatas_all

        # í•„í„°ë§í•  í•™ê³¼ê°€ ì§€ì •ëœ ê²½ìš°
        if filter_major:
            print(f"ğŸ” '{filter_major}' í•™ê³¼ ê´€ë ¨ ë¬¸ì„œë¡œ í•„í„°ë§ ì¤‘...")
            filtered_indices = [
                i for i, meta in enumerate(self.metadatas_all)
                if meta and meta.get('major') == filter_major
            ]
            if not filtered_indices:
                print(f"âš ï¸ '{filter_major}' í•™ê³¼ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            else:
                current_corpus = [self.corpus_all[i] for i in filtered_indices]
                current_metadatas = [self.metadatas_all[i] for i in filtered_indices]
                print(f"ğŸ” í•„í„°ë§ ê²°ê³¼: ì´ {len(current_corpus)}ê°œì˜ ë¬¸ì„œë¡œ ì œí•œë¨.")


        if not current_corpus: # í•„í„°ë§ í›„ ë¬¸ì„œê°€ ì—†ì„ ê²½ìš°
             print("âš ï¸ ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
             return []

        # BM25 ê³„ì‚° (í•„í„°ë§ëœ ì½”í¼ìŠ¤ ë˜ëŠ” ì „ì²´ ì½”í¼ìŠ¤ ëŒ€ìƒ)
        tokenized_corpus = [doc.split() for doc in current_corpus]
        if not tokenized_corpus: # í† í°í™”ëœ ì½”í¼ìŠ¤ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ì²˜ë¦¬
            print("âš ï¸ í† í°í™”ëœ ë¬¸ì„œê°€ ì—†ì–´ BM25 ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []
        
        bm25 = BM25Okapi(tokenized_corpus)
        tokenized_query = query.split()
        bm25_scores = bm25.get_scores(tokenized_query)

        # BM25 ê²°ê³¼ê°€ top_k_bm25ë³´ë‹¤ ì ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„
        num_bm25_candidates = min(top_k_bm25, len(current_corpus))
        bm25_indices_in_current = np.argsort(bm25_scores)[::-1][:num_bm25_candidates]

        # í˜„ì¬ ì½”í¼ìŠ¤(í•„í„°ë§ë˜ì—ˆì„ ìˆ˜ ìˆìŒ) ë‚´ì—ì„œì˜ ì¸ë±ìŠ¤ì´ë¯€ë¡œ,
        # ì›ë˜ ì½”í¼ìŠ¤ì—ì„œì˜ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•  í•„ìš”ëŠ” ì—†ìŒ.
        # ë°”ë¡œ í˜„ì¬ ì½”í¼ìŠ¤ì—ì„œ í•´ë‹¹ ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´.
        bm25_candidates_docs = [current_corpus[i] for i in bm25_indices_in_current]
        bm25_candidates_meta = [current_metadatas[i] for i in bm25_indices_in_current]

        if not bm25_candidates_docs:
            print("âš ï¸ BM25 ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # Dense retrieval (ì˜ë¯¸ë¡ ì  ê²€ìƒ‰)
        # í•„í„°ë§ëœ ë¬¸ì„œë“¤ì˜ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ê±°ë‚˜, í•„ìš”ì‹œ ì¦‰ì„ì—ì„œ ê³„ì‚°
        # ì „ì²´ ë¬¸ì„œ ì„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°í•´ë‘ê³  í•„í„°ë§ëœ ì¸ë±ìŠ¤ë¡œ ê°€ì ¸ì˜¤ëŠ” ë°©ë²•ë„ ìˆìŒ
        # self.dense_embeddings_all ì‚¬ìš© ì‹œ:
        # if filter_major and filtered_indices:
        #     candidate_embeddings = self.dense_embeddings_all[filtered_indices_for_dense] # ì£¼ì˜: ì¸ë±ìŠ¤ ë§¤ì¹­ í•„ìš”
        # else:
        #     candidate_embeddings = self.encoder.encode(bm25_candidates_docs) # í˜„ì¬ ë°©ì‹: BM25 ê²°ê³¼ì— ëŒ€í•´ì„œë§Œ ì¸ì½”ë”©
        
        candidate_embeddings = self.encoder.encode(bm25_candidates_docs)
        query_embedding = self.encoder.encode([query])
        similarities = cosine_similarity(query_embedding, candidate_embeddings)[0]

        # DPR ê²°ê³¼ê°€ top_k_dprë³´ë‹¤ ì ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„
        num_dpr_candidates = min(top_k_dpr, len(bm25_candidates_docs))
        top_indices_in_bm25 = np.argsort(similarities)[::-1][:num_dpr_candidates]

        final_results = [
            (bm25_candidates_docs[i], bm25_candidates_meta[i]) for i in top_indices_in_bm25
        ]
        return final_results

# âœ… GPT ì‘ë‹µ ìƒì„±ê¸°
def generate_answer(query, context_docs):
    if not context_docs: # ì°¸ê³  ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ì£¼ì„¸ìš”."

    context = "\n\n".join([doc for doc, meta in context_docs])

    messages = [
        {"role": "system", "content": "ë‹¤ìŒ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”. ë‹¤ìŒ í…ìŠ¤íŠ¸ëŠ” 2025ë…„ë„ ì„œê°•ëŒ€í•™êµ í•™ì‚¬ìš”ëŒì—ì„œ ì¶”ì¶œí•œ ê° í•™ê³¼ ë° ê³¼ëª©ë³„ ì •ë³´ì…ë‹ˆë‹¤. ë§Œì•½ ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´, ì•„ëŠ” ì„ ì—ì„œ ìµœëŒ€í•œ ë‹µë³€í•˜ê±°ë‚˜ ì¶”ê°€ ì •ë³´ê°€ í•„ìš”í•˜ë‹¤ê³  ì–¸ê¸‰í•˜ì„¸ìš”."},
        {"role": "user", "content": f"{context}\n\nì§ˆë¬¸: {query}\në‹µë³€:"}
    ]

    total_tokens = count_total_tokens(messages, model="gpt-4o")
    max_tokens_model = 128000 # ëª¨ë¸ì˜ ìµœëŒ€ í† í° (gpt-4o ê¸°ì¤€)
    max_response_tokens = 4096 # ë‹µë³€ìœ¼ë¡œ ë°›ê³ ì í•˜ëŠ” ìµœëŒ€ í† í° ìˆ˜

    # í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ ê²½ìš°, ëª¨ë¸ì˜ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ìë¥´ê±°ë‚˜,
    # ë‹µë³€ ìƒì„± í† í° ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ ì…ë ¥ í† í°ì„ ì¡°ì ˆí•´ì•¼ í•¨.
    # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ ê²½ê³ ë§Œ ì¶œë ¥
    if total_tokens > max_tokens_model - max_response_tokens : # ëª¨ë¸ í•œê³„ - ì‘ë‹µ í† í° = í”„ë¡¬í”„íŠ¸ ìµœëŒ€
        print(f"âš ï¸ ì „ì²´ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜ê°€ {total_tokens}ê°œë¡œ ëª¨ë¸ ì œí•œ({max_tokens_model})ì„ ì´ˆê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë¶€ ì»¨í…ìŠ¤íŠ¸ê°€ ì˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        # ì‹¤ì œë¡œëŠ” contextë¥¼ ì¤„ì´ëŠ” ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=max_response_tokens # ë‹µë³€ í† í° ìˆ˜ ì œí•œ
        )
        return response.choices[0].message['content'] # ìˆ˜ì •: .message.content -> .message['content']
    except openai.error.InvalidRequestError as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # í† í° ì´ˆê³¼ ì—ëŸ¬ì˜ ê²½ìš°, ì—¬ê¸°ì„œ contextë¥¼ ì¤„ì—¬ì„œ ì¬ì‹œë„í•˜ëŠ” ë¡œì§ì„ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        if "maximum context length" in str(e):
            return "ì§ˆë¬¸ê³¼ ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” ì§§ê²Œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜, í•„í„°ë§ì„ í†µí•´ ë¬¸ì„œ ë²”ìœ„ë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”."
        return "ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    except Exception as e: # ë‹¤ë¥¸ ì˜ˆì™¸ ì²˜ë¦¬
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


def preprocess_query(query):
    for full_name, abbr_list in ABBREVIATION_GROUPS.items():
        for abbr in abbr_list:
            if abbr in query:
                query = query.replace(abbr, full_name)
    return query

# âœ… ë©”ì¸ ì‹¤í–‰ ë£¨í”„
if __name__ == "__main__":
    print("ğŸ’¬ í•™ì‚¬ìš”ëŒ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì‹œì‘ë¨. ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'exit')")

    corpus, metadatas, unique_majors = load_corpus()
    if not corpus:
        print("âš ï¸ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. DBë¥¼ ë¨¼ì € ìƒì„±í•˜ê±°ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()
        
    print(f"ğŸ“š ì´ {len(unique_majors)}ê°œì˜ í•™ê³¼ ì •ë³´ ë¡œë“œë¨: {unique_majors[:10]} ë“±") # ì²˜ìŒ 10ê°œ í•™ê³¼ë§Œ ì¶œë ¥
    retriever = HybridRetriever(corpus, metadatas)

    while True:
        query = input("\nâ“ ì§ˆë¬¸: ")
        if query.lower().strip() == "exit":
            break

        # 1) ì¶•ì•½ì–´ ê·¸ë£¹ ì¹˜í™˜ ì ìš©
        query = preprocess_query(query)

        # 2) ë³€í™˜ëœ ì§ˆì˜ë¡œ í•™ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
        major_filter_keyword = extract_major_keyword(query, unique_majors)

        if major_filter_keyword:
            print(f"âœ¨ '{major_filter_keyword}' ê´€ë ¨ ì •ë³´ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        else:
            print("â„¹ï¸ íŠ¹ì • í•™ê³¼ í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

        # 3) í•„í„°ë§ í‚¤ì›Œë“œë¥¼ retrieverì— ì „ë‹¬
        top_docs_with_meta = retriever.retrieve(query, top_k_bm25=10, top_k_dpr=3, filter_major=major_filter_keyword)

        if not top_docs_with_meta:
            print("\nğŸ§  GPT ì‘ë‹µ:\nê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            continue # ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ

        answer = generate_answer(query, top_docs_with_meta)
        print(f"\nğŸ§  GPT ì‘ë‹µ:\n{answer}")

        print("\nğŸ“ ì°¸ê³ í•œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:")
        for doc_content, meta in top_docs_with_meta: # ë¬¸ì„œ ë‚´ìš©ë„ í•¨ê»˜ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            # print(f" - (ë‚´ìš© ì¼ë¶€: {doc_content[:50]}...) ë©”íƒ€ë°ì´í„°: {meta}")
            print(f" - ë©”íƒ€ë°ì´í„°: {meta}")