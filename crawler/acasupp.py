import requests
import json
import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
from io import BytesIO
import PyPDF2


# PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ í´ë˜ìŠ¤
class PDFExtractor:
    @staticmethod
    def extract_text_from_pdf(pdf_url):
        try:
            response = requests.get(pdf_url, verify=False)
            pdf_file = BytesIO(response.content)

            pdf_reader = PyPDF2.PdfReader(pdf_file)
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text()
            return text.strip()
        except Exception as e:
            print(f"âš ï¸ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return ""


# ê²Œì‹œê¸€ ì„¸ë¶€ ë‚´ìš© ì¶”ì¶œ í´ë˜ìŠ¤
class PostDetailExtractor:
    def __init__(self, driver, base_url):
        self.driver = driver
        self.base_url = base_url

    # ê²Œì‹œê¸€ì—ì„œ PDF ë§í¬ë¥¼ ì¶”ì¶œ
    def extract_pdf_link(self, soup):
        # PDF ë§í¬ê°€ iframe ë‚´ì— í¬í•¨ë˜ì–´ ìˆì„ ìˆ˜ ìˆìŒ
        iframe_tag = soup.select_one("iframe[src*='viewer.html?file']")
        if iframe_tag:
            iframe_src = iframe_tag.get('src')
            pdf_link = iframe_src.split("file=")[-1]
            pdf_link = requests.utils.unquote(pdf_link)  # URL ë””ì½”ë”©
            if not pdf_link.startswith("http"):
                pdf_link = self.base_url + pdf_link
            return pdf_link
        
        # ì¼ë°˜ì ìœ¼ë¡œ a íƒœê·¸ì—ì„œ .pdf íŒŒì¼ ë§í¬ ì¶”ì¶œ
        pdf_tag = soup.select_one("a[href$='.pdf']")
        if pdf_tag:
            pdf_link = pdf_tag['href']
            if not pdf_link.startswith("http"):
                pdf_link = self.base_url + pdf_link  # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½
            return pdf_link
        return None

    # ê²Œì‹œê¸€ì—ì„œ ì´ë¯¸ì§€ URLì„ ì¶”ì¶œ (íŠ¹ì • img íƒœê·¸ë§Œ ì¶”ì¶œ)
    def extract_image_urls(self, soup):
        img_urls = []
        img_tags = soup.select("p img")  # ì›í•˜ëŠ” <img> íƒœê·¸ë§Œ ì„ íƒ
        
        for img in img_tags:
            img_url = img.get("src")
            
            # .svg ì´ë¯¸ì§€ëŠ” ì œì™¸
            if img_url and not img_url.endswith('.svg'):
                # ìƒëŒ€ ê²½ë¡œì¼ ê²½ìš° ì ˆëŒ€ ê²½ë¡œë¡œ ë³€ê²½
                if not img_url.startswith("http"):
                    img_url = self.base_url + img_url
                img_urls.append(img_url)
        
        return img_urls


    # ê²Œì‹œê¸€ ë‚´ìš© ì¶”ì¶œ (PDF ìˆëŠ” ê²½ìš°, ì—†ëŠ” ê²½ìš° ëª¨ë‘ ì²˜ë¦¬)
    def extract_post_detail(self):
        soup = BeautifulSoup(self.driver.page_source, "html.parser")

        title = self._extract_title(soup)
        author, date, views = self._extract_meta_info(soup)
        content = self._extract_content(soup)

        # PDF ë§í¬ ì¶”ì¶œ
        pdf_link = self.extract_pdf_link(soup)
        pdf_text = ""
        if pdf_link:
            print(f"ğŸ“„ PDF ë‹¤ìš´ë¡œë“œ ì¤‘: {pdf_link}")
            pdf_text = PDFExtractor.extract_text_from_pdf(pdf_link)
            content += "\n\n[PDF ë‚´ìš©]\n" + pdf_text

        # ì´ë¯¸ì§€ URL ì¶”ì¶œ
        image_urls = self.extract_image_urls(soup)

        return {
            "title": title,
            "author": author,
            "date": date,
            "views": views,
            "content": content,
            "url": self.driver.current_url,
            "pdf_url": pdf_link if pdf_link else None,
            "image_urls": image_urls  # ì´ë¯¸ì§€ URL ë¦¬ìŠ¤íŠ¸ ì¶”ê°€
        }

    def _extract_title(self, soup):
        title_tag = soup.select_one("div#set-alt-image")
        return title_tag.text.strip() if title_tag else "ì œëª© ì—†ìŒ"

    def _extract_meta_info(self, soup):
        author, date, views = "", "", ""
        try:
            meta_box = soup.select_one("div.flex.justify-between.md\\:mt-6")
            if meta_box:
                text_blocks = meta_box.find_all("div", recursive=True)
                for i in range(len(text_blocks)):
                    label = text_blocks[i].text.strip()
                    if label == "ì‘ì„±ì":
                        author = text_blocks[i + 2].text.strip()
                    elif label == "ì‘ì„±ì¼":
                        date = text_blocks[i + 2].text.strip()
                    elif label == "ì¡°íšŒìˆ˜":
                        views = text_blocks[i + 2].text.strip()
        except Exception as e:
            print("âš ï¸ ë©”íƒ€ ì •ë³´ íŒŒì‹± ì‹¤íŒ¨:", e)
        return author, date, views

    def _extract_content(self, soup):
        content_container = soup.select_one("div.break-words.custom-css-tag-a.tiptap")
        return "\n".join([p.text.strip() for p in content_container.select("p.zoom-text")]) if content_container else ""


# ì›¹ ë“œë¼ì´ë²„ ê´€ë¦¬ í´ë˜ìŠ¤
class WebDriverManager:
    @staticmethod
    def initialize_driver():
        options = webdriver.ChromeOptions()
        #options.add_argument("--headless")  # ì°½ ì•ˆ ë„ìš°ëŠ” ì˜µì…˜
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        return driver

# ë©”ì¸ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤
class NoticeScraper:
    def __init__(self, driver, base_url, start_page=1, end_page=1):
        self.driver = driver
        self.base_url = base_url
        self.start_page = start_page
        self.end_page = end_page
        self.post_detail_extractor = PostDetailExtractor(driver, base_url)

    def scrape_notice_pages(self):
        all_data = []

        for page_num in range(self.start_page, self.end_page + 1):
            print(f"\nğŸ“„ í˜ì´ì§€ {page_num} ì ‘ì† ì¤‘...")
            self.driver.get(f"{self.base_url}?page={page_num}")
            time.sleep(3)

            posts = self.driver.find_elements(By.CSS_SELECTOR, "div.cursor-pointer.border-b-\\[1px\\]")

            for i in range(len(posts)):
                try:
                    post_elements = self.driver.find_elements(By.CSS_SELECTOR, "div.cursor-pointer.border-b-\\[1px\\]")
                    post = post_elements[i]
                    preview_text = post.text.split("\n")[0].strip()
                    print(f"  - ê²Œì‹œê¸€ í´ë¦­ ì¤‘: {preview_text}")

                    self.driver.execute_script("arguments[0].click();", post)
                    time.sleep(1)

                    # ê²Œì‹œê¸€ ì„¸ë¶€ ì •ë³´ ì¶”ì¶œ, ìµœëŒ€ 3ë²ˆ ì‹œë„
                    retry_count = 0
                    data = None
                    while retry_count < 3:
                        try:
                            data = self.post_detail_extractor.extract_post_detail()

                            # ë””ë²„ê¹…ìš©ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¼ë¶€ ì¶œë ¥
                            print(f"ğŸ“„ ì œëª©: {data['title']}")
                            print(f"ğŸ“„ ë‚´ìš© ì¼ë¶€: {data['content'][:200]}...")  # ë³¸ë¬¸ ë‚´ìš©ì˜ ì• 200ì ì¶œë ¥
                            
                            # ì œëª©ê³¼ ë³¸ë¬¸ ë‚´ìš©ì´ ìˆìœ¼ë©´ ë°”ë¡œ ì €ì¥
                            if data['title'] != "ì œëª© ì—†ìŒ" and (data['content'] or data['image_urls']):
                                break  # ë³¸ë¬¸ì´ë‚˜ ì´ë¯¸ì§€ê°€ ìˆë‹¤ë©´ ë©ˆì¶”ê³  ì €ì¥

                        except Exception as e:
                            print(f"    âŒ ê²Œì‹œê¸€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                            retry_count += 1
                            if retry_count >= 3:
                                print("âš ï¸ ìµœëŒ€ ì‹œë„ íšŸìˆ˜ ì´ˆê³¼, ë‹¤ìŒ ê²Œì‹œê¸€ë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤.")
                            time.sleep(2)

                    if data:
                        all_data.append(data)
                    self.driver.back()
                    time.sleep(1)

                except Exception as e:
                    print(f"    âŒ ê²Œì‹œê¸€ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
                    continue

        return all_data


if __name__ == "__main__":
    driver = WebDriverManager.initialize_driver()
    BASE_URL = "https://sogang.ac.kr/ko/academic-support/notices"

    scraper = NoticeScraper(driver, BASE_URL, start_page=1, end_page=11)
    scraped = scraper.scrape_notice_pages()

    with open("data/acasupport/raw/acasupp.json", "w", encoding="utf-8") as f:
        json.dump(scraped, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… ì´ {len(scraped)}ê°œì˜ ê²Œì‹œê¸€ ì €ì¥ ì™„ë£Œ!")
    driver.quit()
