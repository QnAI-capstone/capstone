from config import OPENAI_API_KEY
import openai
import tiktoken
import numpy as np
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from chromadb import PersistentClient
from chromadb.utils.embedding_functions import EmbeddingFunction
import re # ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©ì„ ìœ„í•´ ì¶”ê°€
from rapidfuzz import process
from dictionary import ABBREVIATION_GROUPS,DATE_GROUPS
import math
from ex_sub import extract_subject_by_rapidfuzz
import time
from konlpy.tag import Okt
import pandas as pd
from hybrid_rag import load_corpus_by_collection, extract_major_keyword, preprocess_query, KoSimCSEEmbedding,count_total_tokens
from openpyxl import Workbook

# âœ… API í‚¤
openai.api_key = OPENAI_API_KEY
# âœ… GPT ì‘ë‹µ ìƒì„±ê¸°
def generate_answer(query, context_docs):
    
    if not context_docs: # ì°¸ê³  ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ì£¼ì„¸ìš”."
    
    
    context = context_docs

    prompt = (
        "ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµì˜ í•™ì‚¬ ìš”ëŒ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n"
        "- ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë”ë¼ë„, ê´€ë ¨ í•™ê³¼ ë˜ëŠ” ê·œì • ë¬¸ì„œë¥¼ ëª¨ë‘ ì°¸ê³ í•˜ì—¬ ê°€ëŠ¥í•œ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”.\n"
        "- ê°™ì€ ê³¼ëª©ì— ëŒ€í•œ ì„¤ëª…ì´ ì—¬ëŸ¬ í•™ê³¼ ë˜ëŠ” ì „ê³µì—ì„œ ë°˜ë³µë  ê²½ìš°, **ëª¨ë“  ê´€ë ¨ ë¬¸ì„œì—ì„œ ë‚˜ì˜¨ ì„¤ëª…ì„ ë¹ ì§ì—†ì´ í¬í•¨**í•˜ì„¸ìš”.\n"
        "- ê°ê°ì˜ ì„¤ëª…ì€ **ì¶œì²˜ í•™ê³¼ëª… ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ì„ ë¶„ë¦¬í•˜ì—¬ ì¶œë ¥**í•˜ê³ , ì¤‘ë³µëœ ë‚´ìš©ì´ ìˆë”ë¼ë„ **í•™ê³¼ ë¬¸ë§¥ ë‚´ì—ì„œëŠ” ìƒëµí•˜ì§€ ë§ê³  ëª¨ë‘ ì¶œë ¥**í•˜ì„¸ìš”.\n"
        "- ìš”ì•½í•˜ì§€ ë§ˆì„¸ìš”. **ëª¨ë“  í•™ê³¼ë³„ ì„¤ëª…ì„ ì „ë¶€ ë‚˜ì—´**í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n"
        "- ì œê³µëœ contextì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ê²½ìš°, \"ì œê³µëœ ì •ë³´ì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"ë¼ê³  ì¶œë ¥í•˜ì„¸ìš”.\n"
        )

    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": f"context:\n{context}\n\nì§ˆë¬¸: {query}\në‹µë³€:"}
    ]
    model_name="gpt-4o"


    total_tokens = count_total_tokens(messages, model="gpt-4o")
    max_tokens_model = 128000 # ëª¨ë¸ì˜ ìµœëŒ€ í† í° (gpt-4o ê¸°ì¤€)
    max_response_tokens = 4096 # ë‹µë³€ìœ¼ë¡œ ë°›ê³ ì í•˜ëŠ” ìµœëŒ€ í† í° ìˆ˜

    # í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ ê²½ìš°, ëª¨ë¸ì˜ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ìë¥´ê±°ë‚˜,
    # ë‹µë³€ ìƒì„± í† í° ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ ì…ë ¥ í† í°ì„ ì¡°ì ˆí•´ì•¼ í•¨.
    # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ ê²½ê³ ë§Œ ì¶œë ¥
    if total_tokens > max_tokens_model - max_response_tokens : # ëª¨ë¸ í•œê³„ - ì‘ë‹µ í† í° = í”„ë¡¬í”„íŠ¸ ìµœëŒ€
        print(f"âš ï¸ ì „ì²´ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜ê°€ {total_tokens}ê°œë¡œ ëª¨ë¸ ì œí•œ({max_tokens_model})ì„ ì´ˆê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë¶€ ì»¨í…ìŠ¤íŠ¸ê°€ ì˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        # ì‹¤ì œë¡œëŠ” contextë¥¼ ì¤„ì´ëŠ” ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.
    try:
        response = openai.ChatCompletion.create(
            model=model_name,
            messages=messages,
            max_tokens=4096, # ë‹µë³€ í† í° ìˆ˜ ì œí•œ
            temperature = 0.3,
            top_p = 0.9
        )
        return response.choices[0].message['content'] # ìˆ˜ì •: .message.content -> .message['content']
    except openai.error.InvalidRequestError as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # í† í° ì´ˆê³¼ ì—ëŸ¬ì˜ ê²½ìš°, ì—¬ê¸°ì„œ contextë¥¼ ì¤„ì—¬ì„œ ì¬ì‹œë„í•˜ëŠ” ë¡œì§ì„ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        if "maximum context length" in str(e):
            return "ì§ˆë¬¸ê³¼ ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” ì§§ê²Œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜, í•„í„°ë§ì„ í†µí•´ ë¬¸ì„œ ë²”ìœ„ë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”."
        return "ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    except Exception as e: # ë‹¤ë¥¸ ì˜ˆì™¸ ì²˜ë¦¬
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

# âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
class HybridRetriever:
    def __init__(self, corpus_all, metadatas_all,collection_name):
        # ì´ˆê¸°í™” ì‹œì ì—ëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ë³´ê´€
        self.corpus_all = corpus_all
        self.metadatas_all = metadatas_all
        self.encoder = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
        self.collection_name = collection_name

    def retrieve(self, query, top_k = 10, filter_major=None,alpha=0.5,cat=1):
        bm25_rows, dpr_rows = [], []
        hybrid_rows = []
        flag = 0
        query_bm25 = query
        # ì‹¤ì œ ê²€ìƒ‰ ëŒ€ìƒì´ ë  ì½”í¼ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°
        current_corpus = self.corpus_all
        current_metadatas = self.metadatas_all

        # í•„í„°ë§í•  í•™ê³¼ê°€ ì§€ì •ëœ ê²½ìš°
        if filter_major:
            print(f"ğŸ” '{filter_major}' í•™ê³¼ ê´€ë ¨ ë¬¸ì„œë¡œ í•„í„°ë§ ì¤‘...")
            filtered_indices = [
                i for i, meta in enumerate(self.metadatas_all)
                if meta and meta.get('major') in filter_major #filter_majorë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°”ê¿ˆ
            ]
            flag = 1
            if not filtered_indices:
                print(f"âš ï¸ '{filter_major}' í•™ê³¼ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            else:
                current_corpus = [self.corpus_all[i] for i in filtered_indices]
                current_metadatas = [self.metadatas_all[i] for i in filtered_indices]
                print(f"ğŸ” í•„í„°ë§ ê²°ê³¼: ì´ {len(current_corpus)}ê°œì˜ ë¬¸ì„œë¡œ ì œí•œë¨.")


        if not current_corpus: # í•„í„°ë§ í›„ ë¬¸ì„œê°€ ì—†ì„ ê²½ìš°
             print("âš ï¸ ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
             return []
        
        if flag == 0:
            sub = extract_subject_by_rapidfuzz(query)
            query_bm25 = query.strip()+" "+sub[0]+" ê³¼ëª©"

        # BM25 ê³„ì‚° (í•„í„°ë§ëœ ì½”í¼ìŠ¤ ë˜ëŠ” ì „ì²´ ì½”í¼ìŠ¤ ëŒ€ìƒ)
        tokenized_corpus = [doc.split() for doc in current_corpus]
        
        bm25 = BM25Okapi(tokenized_corpus,b=0.25)
        tokenized_query = query_bm25.split()
        print(query_bm25)
        bm25_scores = bm25.get_scores(tokenized_query)
        bm25_norm = (bm25_scores - np.min(bm25_scores)) / (np.max(bm25_scores) - np.min(bm25_scores) + 1e-8)

        # BM25 ê²°ê³¼ê°€ top_k_bm25ë³´ë‹¤ ì ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„
        num_bm25_candidates = min(6, len(current_corpus))
        bm25_indices_in_current = np.argsort(bm25_scores)[::-1][:num_bm25_candidates]
        bm25_candidates_docs = [current_corpus[i] for i in bm25_indices_in_current]
        bm25_candidates_meta = [current_metadatas[i] for i in bm25_indices_in_current]

        doc_gpt_blocks = []  # ê° ë¬¸ì„œ ë¸”ë¡ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸

        for rank, i in enumerate(bm25_indices_in_current):
            bm25_rows.append({
                "rank": rank + 1,
                "score": bm25_norm[i],
                "meta": current_metadatas[i],
                "doc_full": current_corpus[i].replace("\n", " ")
            })

            # ë¬¸ì„œ ë¸”ë¡ í¬ë§· ìƒì„± ë° ëˆ„ì 
            meta = current_metadatas[i]
            doc = current_corpus[i]
            print(doc[:50])
            block = f"[{meta.get('university', 'Unknown University')} - {meta.get('major', 'Unknown Major')} - {meta.get('source_file', 'Unknown Source')}]\n{doc}"
            doc_gpt_blocks.append(block)

        # ìµœì¢… GPT inputìœ¼ë¡œ ì‚¬ìš©í•  í…ìŠ¤íŠ¸ êµ¬ì„±
        doc_gpt = "\n\n".join(doc_gpt_blocks)


        answer = generate_answer(query, doc_gpt)
        print(f"bm25 answer: {answer}")

        bm25_rows.append(
            {
                "answer": answer
            }
        )

        # -------------------------------
        # 2ï¸âƒ£ ChromaDB ë²¡í„° ë¶ˆëŸ¬ì™€ ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°
        # -------------------------------
        
        embedding_fn = KoSimCSEEmbedding()
        client = PersistentClient(path="./chroma_store")
        collection = client.get_collection(name=self.collection_name, embedding_function=embedding_fn)

        retrieved = collection.get(include=["embeddings", "metadatas", "documents"])
        all_embeddings = retrieved["embeddings"]
        all_metadatas = retrieved["metadatas"]
        all_documents = retrieved["documents"]

        # 2ï¸âƒ£ ì§ˆì˜ ì„ë² ë”©
        query_embedding = self.encoder.encode([query])  # (1, dim)

        # 3ï¸âƒ£ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì „ì²´ ë¬¸ì„œ ëŒ€ìƒ)
        similarities = cosine_similarity(query_embedding, all_embeddings)[0]  # (num_docs,)

        dpr_candidate = min(6, len(similarities))
        top_indices = np.argsort(similarities)[::-1][:dpr_candidate]
        dpr_candidates_docs = [all_documents[i] for i in top_indices]
        dpr_candidates_meta = [all_metadatas[i] for i in top_indices]

        # ì¤‘ë³µ ì œê±°ëœ DPR ê²°ê³¼
        dpr_indices_unique = [i for i in top_indices if all_documents[i] not in bm25_candidates_docs]

        doc_gpt_blocks = []  # ê° ë¬¸ì„œ ë¸”ë¡ì„ ë‹´ì„ ë¦¬ìŠ¤íŠ¸

        for rank, i in enumerate(top_indices):
            dpr_rows.append({
                "rank": rank + 1,
                "score": similarities[i],
                "meta": all_metadatas[i],
                "doc_full": all_documents[i].replace("\n", " ")
            })

            # ë¬¸ì„œ ë¸”ë¡ í¬ë§· ìƒì„± ë° ëˆ„ì 
            meta = all_metadatas[i]
            doc = all_documents[i]
            block = f"[{meta.get('university', 'Unknown University')} - {meta.get('major', 'Unknown Major')} - {meta.get('source_file', 'Unknown Source')}]\n{doc}"
            doc_gpt_blocks.append(block)

        # ìµœì¢… GPT inputìœ¼ë¡œ ì‚¬ìš©í•  í…ìŠ¤íŠ¸ êµ¬ì„±
        doc_gpt = "\n\n".join(doc_gpt_blocks)

        answer = generate_answer(query, doc_gpt)
        print(f"answer: {answer}")

        dpr_rows.append(
            {
                "answer": answer
            }
        )

        '''doc_gpt_blocks = []

        for i in range(3):
            if i < len(bm25_indices_in_current):
                doc = bm25_candidates_docs[i]
                meta = bm25_candidates_meta[i]
                hybrid_rows.append({"rank": len(hybrid_rows)+1, "doc": doc, "meta": meta})
                doc_gpt_blocks.append(f"[{meta.get('university', '')} - {meta.get('major', '')} - {meta.get('source_file', '')}]\n{doc}")

            if i < len(dpr_indices_unique):
                dpr_doc = all_documents[dpr_indices_unique[i]]
                dpr_meta = all_metadatas[dpr_indices_unique[i]]
                hybrid_rows.append({"rank": len(hybrid_rows)+1, "doc": dpr_doc, "meta": dpr_meta})
                doc_gpt_blocks.append(f"[{dpr_meta.get('university', '')} - {dpr_meta.get('major', '')} - {dpr_meta.get('source_file', '')}]\n{dpr_doc}")

        doc_gpt = "\n\n".join(doc_gpt_blocks)
        answer = generate_answer(query, doc_gpt)
        print(f"answer: {answer}")

        hybrid_rows.append({"answer": answer})'''


        return bm25_rows,dpr_rows

print("ğŸ’¬ í•™ì‚¬ìš”ëŒ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì‹œì‘ë¨.")

collection_data = load_corpus_by_collection()

# âœ… ì»¬ë ‰ì…˜ë³„ retriever ì´ˆê¸°í™”
retrievers = {}
for col_name, content in collection_data.items():
    retrievers[col_name] = HybridRetriever(
        content["documents"],
        content["metadatas"],
        collection_name=col_name
    )

# âœ… major ëª©ë¡ë„ í•¨ê»˜ ì €ì¥
majors_by_collection = {
    col_name: content["majors"]
    for col_name, content in collection_data.items()
    if "majors" in content
}   

# âœ… ì¹´í…Œê³ ë¦¬ â†’ ì»¬ë ‰ì…˜ ì´ë¦„ ë§¤í•‘
category_to_collection = {
    "1": "collection_course",
    "2": "collection_subjectinfo_chunk",
    "3": "collection_notice_md"
}

def overall_logic(query, cat):
    selected_collection = category_to_collection[cat]

    retriever = retrievers[selected_collection]

    if selected_collection != "collection_notice_md":
        unique_majors = majors_by_collection[selected_collection]

    if selected_collection == "collection_notice_md":
        #ì‹œê¸° ì¶”ì¶œ
        date_keyword = extract_date_key_from_query(query)
            

        # ê¸°ë³¸ê°’ ì„¤ì •
        if date_keyword is None:
            print("â„¹ï¸ íŠ¹ì • ì‹œê¸° í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. '2025-1' ë¬¸ì„œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            date_keyword = "2025-1"
        else:
            print(f"âœ¨ '{date_keyword}' ê´€ë ¨ ì •ë³´ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

        query = query.strip()+" "+date_keyword+"ì™€ ê´€ë ¨ëœ ê³µì§€ë¥¼ ì°¾ì•„ì¤˜."

        # âœ… í•´ë‹¹ ì‹œê¸°ì˜ ë¬¸ì„œë§Œ í•„í„°ë§
        all_docs = collection_data[selected_collection]["documents"]
        all_metas = collection_data[selected_collection]["metadatas"]

        return None

    elif selected_collection == "collection_subjectinfo_chunk":
        # 1) ì¶•ì•½ì–´ ê·¸ë£¹ ì¹˜í™˜ ì ìš©
        query = preprocess_query(query)

        # 2) ë³€í™˜ëœ ì§ˆì˜ë¡œ í•™ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
        major_filter_keyword = extract_major_keyword(query, unique_majors,threshold = 70)

        if major_filter_keyword:
            print(f"âœ¨ '{major_filter_keyword}' ê´€ë ¨ ì •ë³´ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        else:
            print("â„¹ï¸ íŠ¹ì • í•™ê³¼ í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

        # 3) í•„í„°ë§ í‚¤ì›Œë“œë¥¼ retrieverì— ì „ë‹¬
        bm25_rows,dpr_rows = retriever.retrieve(query, top_k = 6, filter_major=major_filter_keyword,alpha=0.5,cat= 2)
        print("ok")
        return bm25_rows,dpr_rows

    else:
        # 1) ì¶•ì•½ì–´ ê·¸ë£¹ ì¹˜í™˜ ì ìš©
        query = preprocess_query(query)

        print(f"query: {query}")

        # 2) ë³€í™˜ëœ ì§ˆì˜ë¡œ í•™ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
        major_filter_keyword = extract_major_keyword(query, unique_majors,threshold = 60)

        if major_filter_keyword:
            print(f"âœ¨ '{major_filter_keyword}' ê´€ë ¨ ì •ë³´ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        else:
            print("â„¹ï¸ íŠ¹ì • í•™ê³¼ í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        

        # 3) í•„í„°ë§ í‚¤ì›Œë“œë¥¼ retrieverì— ì „ë‹¬
        bm25_rows, dpr_rows, hybrid_rows = retriever.retrieve(query, top_k=4, filter_major=major_filter_keyword,cat=1)

        return bm25_rows, dpr_rows, hybrid_rows

if __name__ == "__main__":
    
    # ì—‘ì…€ íŒŒì¼ ê²½ë¡œ
    file_path = "data/testdata.xlsx"  # ì‹¤ì œ íŒŒì¼ ê²½ë¡œë¡œ ë°”ê¿”ì£¼ì„¸ìš”

    # 1. ì—‘ì…€ íŒŒì¼ ì½ê¸°
    df = pd.read_excel(file_path)

    all_bm25, all_dpr, all_hybrid = [], [], []

    # 2. queryì™€ cat ì»¬ëŸ¼ ìˆœíšŒí•˜ë©´ì„œ í•¨ìˆ˜ í˜¸ì¶œ
    for idx, row in df.iterrows():
        query = str(row["query"])
        #cat = str(row["cat"])  # intë¡œ ë³€í™˜í•´ì„œ ì „ë‹¬

        print(f"\nğŸŸ¢ [{idx+1}] Query: {query}")
        
        try:
            bm25_rows,dpr_rows = overall_logic(query, cat = "2")
            #bm25_rows = overall_logic(query, cat = "2")

            # ê° ê²°ê³¼ì— ì›ë˜ query ì •ë³´ë„ í•¨ê»˜ ì €ì¥
            for r in bm25_rows:
                r["query"] = query
                r["method"] = "bm25"
            for r in dpr_rows:
                r["query"] = query
                r["method"] = "DPR"

            all_bm25.extend(bm25_rows)
            all_dpr.extend(dpr_rows)

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue

    # 4. DataFrameìœ¼ë¡œ ë³€í™˜
    df_bm25 = pd.DataFrame(all_bm25)
    df_dpr = pd.DataFrame(all_dpr)

    # 5. Excelë¡œ ì €ì¥ (ì‹œíŠ¸ë³„ ë¶„ë¦¬)
    with pd.ExcelWriter("retrieval_results13.xlsx", engine="openpyxl") as writer:
        df_bm25.to_excel(writer, sheet_name="hybrid", index=False)
        df_dpr.to_excel(writer, sheet_name="DPR", index=False)
