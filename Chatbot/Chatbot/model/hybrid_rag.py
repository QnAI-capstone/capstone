import os
import sys
import django
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'Chatbot.settings')
django.setup()
from django.conf import settings
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
CHROMA_STORE_PATH = os.path.join(BASE_DIR, 'chroma_store')

import openai
import tiktoken
import numpy as np
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from chromadb import PersistentClient
from chromadb.utils.embedding_functions import EmbeddingFunction
import re # ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©ì„ ìœ„í•´ ì¶”ê°€
from rapidfuzz import process
from scipy.stats import rankdata
from Chatbot.model.dictionary import ABBREVIATION_GROUPS,DATE_GROUPS
from Chatbot.model.ex_sub import extract_subject_by_rapidfuzz
from django.contrib.auth.models import User

# âœ… API í‚¤
openai.api_key = settings.OPENAI_API_KEY

# âœ… í•œêµ­ì–´ ì„ë² ë”© í•¨ìˆ˜
class KoSimCSEEmbedding(EmbeddingFunction):
    def __init__(self):
        self.model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
    def __call__(self, input):
        return self.model.encode(input).tolist()

def load_corpus_by_collection():
    """
    ChromaDBì˜ ëª¨ë“  ì»¬ë ‰ì…˜ì„ ìˆœíšŒí•˜ì—¬,
    ê° ì»¬ë ‰ì…˜ ì´ë¦„ì„ keyë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•´ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        dict[str, dict]: {
            collection_name: {
                "documents": [...],
                "metadatas": [...],
                "majors": [...]
            },
            ...
        }
    """
    client = PersistentClient(path="./chroma_store")
    embedding_fn = KoSimCSEEmbedding()
    collections_info = client.list_collections()

    print(f"ì´ {len(collections_info)}ê°œì˜ ì»¬ë ‰ì…˜ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")

    result = {}

    for col_info in collections_info:
        collection_name = col_info.name
        collection = client.get_collection(name=collection_name, embedding_function=embedding_fn)
        data = collection.get(include=["documents", "metadatas"])

        documents = data["documents"]
        metadatas = data["metadatas"]

        #ê³µì§€ ë°ì´í„° ì²˜ë¦¬
        if collection_name == "collection_notice":
            dates = list({meta.get("date") for meta in metadatas if meta and "date" in meta})
            result[collection_name] = {
                "documents": documents,
                "metadatas": metadatas,
                "dates": dates
            }
            print(f"âœ… 'collection_notice' ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ. ë¬¸ì„œ ìˆ˜: {len(documents)}")

        #ê³¼ëª© ì´ìˆ˜, ê³¼ëª© ì„¤ëª… ë°ì´í„° ì²˜ë¦¬
        else:
            majors = list({meta["major"] for meta in metadatas if meta and "major" in meta})
            result[collection_name] = {
                "documents": documents,
                "metadatas": metadatas,
                "majors": majors
            }

            print(f"âœ… '{collection_name}' ì»¬ë ‰ì…˜ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ. ë¬¸ì„œ ìˆ˜: {len(documents)}")

    return result

def count_total_tokens(messages, model="gpt-4o"):
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")

    total_tokens = 0
    for message in messages:
        total_tokens += 4
        for key, value in message.items():
            total_tokens += len(encoding.encode(value))
    total_tokens += 2
    return total_tokens

# âœ… ì‚¬ìš©ì ì§ˆì˜ì—ì„œ í•™ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_major_keyword(query, majors_list, threshold=70):
    """
    ì‚¬ìš©ì ì§ˆì˜ì—ì„œ ì–¸ê¸‰ëœ í•™ê³¼ í‚¤ì›Œë“œë¥¼ majors_list (DBì— ì €ì¥ëœ ì‹¤ì œ í•™ê³¼ëª…) ê¸°ì¤€ìœ¼ë¡œ ìœ ì‚¬ ë¬¸ìì—´ ë§¤ì¹­í•˜ì—¬ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë„ì–´ì“°ê¸°, ì˜¤íƒ€, ì¶•ì•½ì–´ ì°¨ì´ ë“±ìœ¼ë¡œ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì•„ë„ ê°€ì¥ ìœ ì‚¬í•œ í•™ê³¼ëª…ì„ ì°¾ì•„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ì‚¬ìš©ì ì§ˆì˜ ì •ê·œí™”: ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜
    normalized_query = query.replace(" ", "").lower()

    # majors_listì˜ í•™ê³¼ëª…ë„ ì •ê·œí™” (ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°, ì†Œë¬¸ì ë³€í™˜)
    candidates = [m.replace("_", "").lower() for m in majors_list]

    # rapidfuzzì˜ extractOneìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ í•™ê³¼ëª…ê³¼ ìœ ì‚¬ë„ ë°˜í™˜
    best_match = process.extractOne(normalized_query, candidates)
        
    # âœ… ìœ ì‚¬í•œ í•­ëª© ì—¬ëŸ¬ ê°œ ì¶”ì¶œ
    matches = process.extract(normalized_query, candidates, limit=2)

    # âœ… ìœ ì‚¬ë„ ê¸°ì¤€ í†µê³¼í•œ í•™ê³¼ë§Œ ë°˜í™˜
    result = []
    for match_str, score, _ in matches:
        idx = candidates.index(match_str)
        print(majors_list[idx])
        if score >= threshold:
            idx = candidates.index(match_str)
            matched_major = majors_list[idx]
            result.append(matched_major)
            print(f"âœ… ìœ ì‚¬ë„ {score} â†’ ë§¤ì¹­ëœ í•™ê³¼: {matched_major}")
        else:
            print(f"âŒ ìœ ì‚¬ë„ {score} â†’ ë¬´ì‹œë¨")
        
    return result  # ìµœëŒ€ top_kê°œì˜ í•™ê³¼ëª… ë°˜í™˜

# âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
class HybridRetriever:
    def __init__(self, corpus_all, metadatas_all, collection_name):
        # ì´ˆê¸°í™” ì‹œì ì—ëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ë³´ê´€
        self.corpus_all = corpus_all
        self.metadatas_all = metadatas_all
        self.encoder = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
        self.collection_name = collection_name
        # ì „ì²´ ë¬¸ì„œì— ëŒ€í•œ ì„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°í•´ë‘˜ ìˆ˜ ìˆìœ¼ë‚˜, í•„í„°ë§ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³ ë ¤
        # self.dense_embeddings_all = self.encoder.encode(self.corpus_all) # í•„ìš” ì‹œ í™œì„±í™”

    def retrieve(self, query, top_k_bm25=10, top_k_dpr=3, filter_major=None, alpha=0.5, cat=1):
        print("ğŸŸ¢ ë¬¸ì„œë¥¼ retrieve í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        # ì‹¤ì œ ê²€ìƒ‰ ëŒ€ìƒì´ ë  ì½”í¼ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°
        current_corpus = self.corpus_all
        current_metadatas = self.metadatas_all
        final_results = []
        query_bm25 = query

        # í•„í„°ë§í•  í•™ê³¼ê°€ ì§€ì •ëœ ê²½ìš°
        if filter_major:
            print(f"ğŸ” '{filter_major}' í•™ê³¼ ê´€ë ¨ ë¬¸ì„œë¡œ í•„í„°ë§ ì¤‘...")
            filtered_indices = [
                i for i, meta in enumerate(self.metadatas_all)
                if meta and meta.get('major') in filter_major #filter_majorë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°”ê¿ˆ
            ]
            if not filtered_indices:
                print(f"âš ï¸ '{filter_major}' í•™ê³¼ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            else:
                current_corpus = [self.corpus_all[i] for i in filtered_indices]
                current_metadatas = [self.metadatas_all[i] for i in filtered_indices]
                print(f"ğŸ” í•„í„°ë§ ê²°ê³¼: ì´ {len(current_corpus)}ê°œì˜ ë¬¸ì„œë¡œ ì œí•œë¨.")

        if not current_corpus: # í•„í„°ë§ í›„ ë¬¸ì„œê°€ ì—†ì„ ê²½ìš°
             print("âš ï¸ ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
             return []

        ###ì¶”ê°€
        if cat == 2:
            sub = extract_subject_by_rapidfuzz(query)
            if sub:
                query_bm25 = query.strip()+" "+sub[0]+" ê³¼ëª©"
        
        print(f"query bm25 : {query_bm25}")
        ###

        # BM25 ê³„ì‚° (í•„í„°ë§ëœ ì½”í¼ìŠ¤ ë˜ëŠ” ì „ì²´ ì½”í¼ìŠ¤ ëŒ€ìƒ)
        tokenized_corpus = [doc.split() for doc in current_corpus]
        
        bm25 = BM25Okapi(tokenized_corpus,b=0.25)
        tokenized_query = query_bm25.split()
        bm25_scores = bm25.get_scores(tokenized_query)

        # BM25 ê²°ê³¼ê°€ top_k_bm25ë³´ë‹¤ ì ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„
        num_bm25_candidates = min(top_k_bm25, len(current_corpus))
        bm25_indices_in_current = np.argsort(bm25_scores)[::-1][:num_bm25_candidates]
        bm25_candidates_docs = [current_corpus[i] for i in bm25_indices_in_current]
        bm25_candidates_meta = [current_metadatas[i] for i in bm25_indices_in_current]

        # -------------------------------
        # 2ï¸âƒ£ cat = 2ì¸ ê²½ìš°ì—ë§Œ ChromaDB ë²¡í„° ë¶ˆëŸ¬ì™€ ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°
        # -------------------------------
        
        if cat == 2:
            embedding_fn = KoSimCSEEmbedding()
            client = PersistentClient(path="./chroma_store")
            collection = client.get_collection(name=self.collection_name, embedding_function=embedding_fn)

            retrieved = collection.get(include=["embeddings", "metadatas", "documents"])
            all_embeddings = retrieved["embeddings"]
            all_metadatas = retrieved["metadatas"]
            all_documents = retrieved["documents"]

            # ì§ˆì˜ ì„ë² ë”©
            query_embedding = self.encoder.encode([query])  # (1, dim)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì „ì²´ ë¬¸ì„œ ëŒ€ìƒ)
            similarities = cosine_similarity(query_embedding, all_embeddings)[0]  # (num_docs,)

            # 4ï¸âƒ£ ìƒìœ„ top_k_dpr ê°œìˆ˜ë§Œí¼ ì¶”ì¶œ
            dpr_candidate = min(top_k_dpr, len(similarities))
            top_indices = np.argsort(similarities)[::-1][:dpr_candidate]

            # ì¤‘ë³µ ì œê±°ëœ DPR ê²°ê³¼
            dpr_indices_unique = [i for i in top_indices if all_documents[i] not in bm25_candidates_docs]

            # ìµœëŒ€ 3ê°œì”© BM25/DPR êµì°¨ ë°°ì¹˜
            for i in range(3):
                if i < len(bm25_indices_in_current):
                    doc = bm25_candidates_docs[i]
                    meta = bm25_candidates_meta[i]
                    final_results.append((doc, meta))
                if i < len(dpr_indices_unique):
                    dpr_doc = all_documents[dpr_indices_unique[i]]
                    dpr_meta = all_metadatas[dpr_indices_unique[i]]
                    final_results.append((dpr_doc, dpr_meta))
            
        else:
            final_results = [
                (bm25_candidates_docs[i], bm25_candidates_meta[i]) for i in bm25_indices_in_current[:num_bm25_candidates]
            ]

        return final_results

# âœ… GPT ì‘ë‹µ ìƒì„±ê¸°
def generate_answer(query, context_docs, log, cat):
    if not context_docs: # ì°¸ê³  ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ì£¼ì„¸ìš”."

    if cat == 1: # collection_course
        context = "\n\n".join([
            f"[{meta.get('university', 'Unknown University')} - {meta.get('major', 'Unknown Major')} - {meta.get('source_file', 'Unknown Source')}]\n{doc}"
            for doc, meta in context_docs
        ])

        prompt = (
            "ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµì˜ í•™ì‚¬ ìš”ëŒì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë”ë¼ë„ ê´€ë ¨ í•™ê³¼ ë˜ëŠ” ê·œì • ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
            "ì´ì „ ëŒ€í™” ë‚´ìš©ì´ ì œê³µëœ ê²½ìš° í•´ë‹¹ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ **ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€**í•˜ì„¸ìš”.\n"
            "ì‚¬ìš©ìê°€ ì´ì „ ì‘ë‹µì„ ì´ì–´ì„œ ì§ˆë¬¸í•  ê²½ìš°(ì˜ˆ: 'ê·¸ì¤‘ì—ì„œ', 'ê·¸ëŸ¬ë©´', 'ì´ì „ì— ë§í•œ ê²ƒ ì¤‘'), ì§ì „ì˜ ì§ˆë¬¸ê³¼ ëª¨ë¸ì˜ ì‘ë‹µ ë‚´ìš©ì„ í•¨ê»˜ ì°¸ê³ í•˜ì—¬ ì¼ê´€ëœ ë§¥ë½ ì†ì—ì„œ ë‹µë³€í•˜ì„¸ìš”. ì´ì „ ì§ˆë¬¸/ì‘ë‹µì€ ì‹œìŠ¤í…œì´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¡œ ì œê³µí•©ë‹ˆë‹¤.\n"
            "ì‚¬ìš©ìëŠ” ì•„ë˜ì˜ ì„¸ ê°€ì§€ ì „ê³µ ìœ í˜• ì¤‘ í•˜ë‚˜ì— í•´ë‹¹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ êµ¬ë¶„ì€ ëª¨ë“  í•™ê³¼ì— ë™ì¼í•˜ê²Œ ì ìš©ë˜ë©°, ì–´ë–¤ ì „ê³µì´ ì£¼ ì „ê³µì¸ì§€ì— ë”°ë¼ í•™ê³¼ë³„ ì¡¸ì—… ìš”ê±´ ë° ì´ìˆ˜ ê¸°ì¤€ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n"
            
            "1. **ë‹¨ì¼ì „ê³µ**: ì‚¬ìš©ìëŠ” íŠ¹ì • í•™ê³¼(ì˜ˆ: ì»´í“¨í„°ê³µí•™ê³¼)ë§Œ ì „ê³µí•©ë‹ˆë‹¤.\n"
            "2. **ë‹¤ì „ê³µ(ìì‹ ì˜ í•™ê³¼)**: ì‚¬ìš©ìëŠ” í•´ë‹¹ í•™ê³¼ë¥¼ ì œ1ì „ê³µìœ¼ë¡œ í•˜ê³ , ë‹¤ë¥¸ í•™ê³¼ë¥¼ ë³µìˆ˜ì „ê³µí•©ë‹ˆë‹¤.\n"
            "3. **ë‹¤ì „ê³µ(íƒ€ í•™ê³¼)**: ì‚¬ìš©ìëŠ” ë‹¤ë¥¸ í•™ê³¼ë¥¼ ì œ1ì „ê³µìœ¼ë¡œ í•˜ê³ , í•´ë‹¹ í•™ê³¼ë¥¼ ë³µìˆ˜ì „ê³µí•©ë‹ˆë‹¤.\n"

            "ì˜ˆì‹œ) ì§ˆë¬¸ì´ ì»´í“¨í„°ê³µí•™ê³¼ì— ëŒ€í•œ ê²ƒì¼ ê²½ìš°:\n"
            "- \"ë‹¨ì¼ì „ê³µ\" ì‚¬ìš©ìëŠ” ì»´í“¨í„°ê³µí•™ê³¼ë§Œ ì „ê³µ\n"
            "- \"ë‹¤ì „ê³µ(ì»´ê³µ)\" ì‚¬ìš©ìëŠ” ì»´í“¨í„°ê³µí•™ê³¼ê°€ ì œ1ì „ê³µ + ë‹¤ë¥¸ í•™ê³¼ ë³µìˆ˜ì „ê³µ\n"
            "- \"ë‹¤ì „ê³µ(íƒ€ì „ê³µ)\" ì‚¬ìš©ìëŠ” ë‹¤ë¥¸ í•™ê³¼ê°€ ì œ1ì „ê³µ + ì»´í“¨í„°ê³µí•™ê³¼ ë³µìˆ˜ì „ê³µ\n"

            "ì§ˆë¬¸ì´ ì–´ëŠ ì „ê³µ ìœ í˜•ì— í•´ë‹¹í•˜ëŠ”ì§€ ëª…í™•í•˜ì§€ ì•Šë”ë¼ë„, ê° ê²½ìš°ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ” ë‚´ìš©ì„ **ëª¨ë‘ ë¶„ë¦¬ëœ ë¬¸ë‹¨**ìœ¼ë¡œ ë‚˜ëˆ  ì„¤ëª…í•˜ì„¸ìš”.\n"
            "- ì œëª©, ì†Œì œëª©, ë¦¬ìŠ¤íŠ¸ ë“±ì„ ì ì ˆíˆ í™œìš©í•˜ì—¬ ì•Œì•„ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬í•˜ì„¸ìš”.\n"
            "ì œê³µëœ contextì—ì„œ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ ì°¾ì„ ìˆ˜ ì—†ë‹¤ê³  ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•´ì£¼ì„¸ìš”.\n"
        )

        messages = [{"role": "system", "content": prompt}]
        check_log(log, messages)
        messages.append({"role": "user", "content": f"context:\n{context}\n\nì§ˆë¬¸: {query}\në‹µë³€:"})
        model_name = "gpt-4o"
    
    elif cat == 2: # collection_subjectinfo
        context = "\n\n".join([
            f"[{meta.get('university', 'Unknown University')} - {meta.get('major', 'Unknown Major')} - {meta.get('source_file', 'Unknown Source')}]\n{doc}"
            for doc, meta in context_docs
        ])

        prompt = (
            "ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµì˜ í•™ì‚¬ ìš”ëŒ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë”ë¼ë„, ê´€ë ¨ í•™ê³¼ ë˜ëŠ” ê·œì • ë¬¸ì„œë¥¼ ëª¨ë‘ ì°¸ê³ í•˜ì—¬ ê°€ëŠ¥í•œ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”.\n"
            "ì´ì „ ëŒ€í™” ë‚´ìš©ì´ ì œê³µëœ ê²½ìš° í•´ë‹¹ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ **ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€**í•˜ì„¸ìš”.\n"
            "ì‚¬ìš©ìê°€ ì´ì „ ì‘ë‹µì„ ì´ì–´ì„œ ì§ˆë¬¸í•  ê²½ìš°(ì˜ˆ: 'ê·¸ì¤‘ì—ì„œ', 'ê·¸ëŸ¬ë©´', 'ì´ì „ì— ë§í•œ ê²ƒ ì¤‘'), ì§ì „ì˜ ì§ˆë¬¸ê³¼ ëª¨ë¸ì˜ ì‘ë‹µ ë‚´ìš©ì„ í•¨ê»˜ ì°¸ê³ í•˜ì—¬ ì¼ê´€ëœ ë§¥ë½ ì†ì—ì„œ ë‹µë³€í•˜ì„¸ìš”. ì´ì „ ì§ˆë¬¸/ì‘ë‹µì€ ì‹œìŠ¤í…œì´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¡œ ì œê³µí•©ë‹ˆë‹¤.\n"
            "ê°™ì€ ê³¼ëª©ì— ëŒ€í•œ ì„¤ëª…ì´ ì—¬ëŸ¬ í•™ê³¼ ë˜ëŠ” ì „ê³µì—ì„œ ë°˜ë³µë  ê²½ìš°, **ëª¨ë“  ê´€ë ¨ ë¬¸ì„œì—ì„œ ë‚˜ì˜¨ ì„¤ëª…ì„ ë¹ ì§ì—†ì´ í¬í•¨**í•˜ì„¸ìš”.\n"
            "ê°ê°ì˜ ì„¤ëª…ì€ **ì¶œì²˜ í•™ê³¼ëª… ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ì„ ë¶„ë¦¬í•˜ì—¬ ì¶œë ¥**í•˜ê³ , ì¤‘ë³µëœ ë‚´ìš©ì´ ìˆë”ë¼ë„ **í•™ê³¼ ë¬¸ë§¥ ë‚´ì—ì„œëŠ” ìƒëµí•˜ì§€ ë§ê³  ëª¨ë‘ ì¶œë ¥**í•˜ì„¸ìš”.\n"
            "ìš”ì•½í•˜ì§€ ë§ˆì„¸ìš”. **ëª¨ë“  í•™ê³¼ë³„ ì„¤ëª…ì„ ì „ë¶€ ë‚˜ì—´**í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n"
            "ì œëª©, ì†Œì œëª©, ë¦¬ìŠ¤íŠ¸ ë“±ì„ ì ì ˆíˆ í™œìš©í•˜ì—¬ ì•Œì•„ë³´ê¸° ì¢‹ê²Œ ì •ë¦¬í•˜ì„¸ìš”.\n"
            "ì œê³µëœ contextì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ê²½ìš°, \"ì œê³µëœ ì •ë³´ì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"ë¼ê³  ì¶œë ¥í•˜ì„¸ìš”.\n"
        )

        messages = [{"role": "system", "content": prompt}]
        check_log(log, messages)
        messages.append({"role": "user", "content": f"context:\n{context}\n\nì§ˆë¬¸: {query}\në‹µë³€:"})
        model_name="gpt-4o"

    else: # collection_notice        
        context = context_docs
        prompt = (
            "ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµì˜ ê³µì§€ì‚¬í•­ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
            "ì´ì „ ëŒ€í™” ë‚´ìš©ì´ ì œê³µëœ ê²½ìš° í•´ë‹¹ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ **ëŒ€í™”ì˜ ë§¥ë½ì„ ìœ ì§€**í•˜ì„¸ìš”.\n"
            "ì‚¬ìš©ìê°€ ì´ì „ ì‘ë‹µì„ ì´ì–´ì„œ ì§ˆë¬¸í•  ê²½ìš°(ì˜ˆ: 'ê·¸ì¤‘ì—ì„œ', 'ê·¸ëŸ¬ë©´', 'ì´ì „ì— ë§í•œ ê²ƒ ì¤‘'), ì§ì „ì˜ ì§ˆë¬¸ê³¼ ëª¨ë¸ì˜ ì‘ë‹µ ë‚´ìš©ì„ í•¨ê»˜ ì°¸ê³ í•˜ì—¬ ì¼ê´€ëœ ë§¥ë½ ì†ì—ì„œ ë‹µë³€í•˜ì„¸ìš”. ì´ì „ ì§ˆë¬¸/ì‘ë‹µì€ ì‹œìŠ¤í…œì´ ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ë¡œ ì œê³µí•©ë‹ˆë‹¤.\n"
            "ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë”ë¼ë„, ì œê³µëœ ê³µì§€ contextë¥¼ ë°”íƒ•ìœ¼ë¡œ ê·œì •ê³¼ ì‚¬ì‹¤ì— ê·¼ê±°í•´ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "ê°€ëŠ¥í•œ í•œ ì§ˆë¬¸ê³¼ í‚¤ì›Œë“œê°€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê³µì§€ë¥¼ ì°¾ì•„ì„œ ì œì‹œí•˜ì„¸ìš”.\n"
            "ì—¬ëŸ¬ ê°œì˜ ê³µì§€ê°€ ê´€ë ¨ ìˆë‹¤ë©´, ë‚ ì§œ(date)ê°€ ê°€ì¥ ìµœì‹ ì¸ ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”.\n"
            "ì œê³µëœ contextì— ê´€ë ¨ ì •ë³´ê°€ ì—†ë‹¤ë©´, 'ê´€ë ¨ ê³µì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n"
            "ë§í¬ëŠ” ë°˜ë“œì‹œ í•œ ë²ˆë§Œ ì¶œë ¥í•˜ê³ , ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì§€ ë§ê³  ìˆœìˆ˜í•œ URLë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n\n"
        )

        messages = [{"role": "system", "content": prompt}]
        check_log(log, messages)
        messages.extend([
            # ğŸŸ¡ One-shot ì˜ˆì‹œ
            {"role": "user", "content": "context:\n[ì¡¸ì—…] 2023í•™ë…„ë„ í›„ê¸°(2024ë…„ 8ì›”) ì¡¸ì—…_í•™ìœ„ì¦ ë°°ë¶€ ë° í•™ìœ„ê°€ìš´ ëŒ€ì—¬ ì•ˆë‚´|2024.07.30|https://sogang.ac.kr/ko/detail/\n\nì§ˆë¬¸: í•™ìœ„ ê°€ìš´ì€ ì–´ë””ì„œ ëŒ€ì—¬í•  ìˆ˜ ìˆì–´?\në‹µë³€:"},
            {"role": "assistant", "content": "í•™ìœ„ ê°€ìš´ ëŒ€ì—¬ì™€ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒ ê³µì§€ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\nì œëª©:[ì¡¸ì—…] 2023í•™ë…„ë„ í›„ê¸°(2024ë…„ 8ì›”) ì¡¸ì—…_í•™ìœ„ì¦ ë°°ë¶€ ë° í•™ìœ„ê°€ìš´ ëŒ€ì—¬ ì•ˆë‚´\nì—…ë¡œë“œì¼ì: 2024.07.30\në§í¬:https://sogang.ac.kr/ko/detail/\n"},
            {"role": "user", "content": f"context:\n{context}\n\nì§ˆë¬¸: {query}\në‹µë³€:"}
        ])
        model_name = "gpt-4o"

    total_tokens = count_total_tokens(messages, model="gpt-4o")
    max_tokens_model = 128000 # ëª¨ë¸ì˜ ìµœëŒ€ í† í° (gpt-4o ê¸°ì¤€)
    max_response_tokens = 4096 # ë‹µë³€ìœ¼ë¡œ ë°›ê³ ì í•˜ëŠ” ìµœëŒ€ í† í° ìˆ˜

    # í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ ê²½ìš°, ëª¨ë¸ì˜ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ìë¥´ê±°ë‚˜,
    # ë‹µë³€ ìƒì„± í† í° ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ ì…ë ¥ í† í°ì„ ì¡°ì ˆí•´ì•¼ í•¨.
    # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ ê²½ê³ ë§Œ ì¶œë ¥
    if total_tokens > max_tokens_model - max_response_tokens : # ëª¨ë¸ í•œê³„ - ì‘ë‹µ í† í° = í”„ë¡¬í”„íŠ¸ ìµœëŒ€
        print(f"âš ï¸ ì „ì²´ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜ê°€ {total_tokens}ê°œë¡œ ëª¨ë¸ ì œí•œ({max_tokens_model})ì„ ì´ˆê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë¶€ ì»¨í…ìŠ¤íŠ¸ê°€ ì˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        # ì‹¤ì œë¡œëŠ” contextë¥¼ ì¤„ì´ëŠ” ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.
    try:
        response = openai.ChatCompletion.create(
            model = model_name,
            messages = messages,
            max_tokens = 4096, # ë‹µë³€ í† í° ìˆ˜ ì œí•œ
            temperature = 0.3,
            top_p = 0.9
        )
        return response.choices[0].message['content']
    except openai.error.InvalidRequestError as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # í† í° ì´ˆê³¼ ì—ëŸ¬ì˜ ê²½ìš°, ì—¬ê¸°ì„œ contextë¥¼ ì¤„ì—¬ì„œ ì¬ì‹œë„í•˜ëŠ” ë¡œì§ì„ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        if "maximum context length" in str(e):
            return "ì§ˆë¬¸ê³¼ ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” ì§§ê²Œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜, í•„í„°ë§ì„ í†µí•´ ë¬¸ì„œ ë²”ìœ„ë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”."
        return "ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    except Exception as e: # ë‹¤ë¥¸ ì˜ˆì™¸ ì²˜ë¦¬
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

#query preprocess module
KOREAN_PARTICLE_PATTERN = r'(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ìœ¼ë¡œ|ë¡œ|ë„|ë§Œ|ê¹Œì§€|ë¶€í„°|ì¡°ì°¨|ì¸ë°|ê³ |ì™€|ê³¼|ë§ˆì €|ì²˜ëŸ¼|ê»˜ì„œ|ë°–ì—|ì´ë©°|ì´ê³ |ì´ë‚˜|ë¼ë„|ë¼ê³ |ë¼ëŠ”|ë“ ì§€|ë§Œí¼)?'

def preprocess_query(query):
    used_majors = []
    replaced_ranges = []  # ì´ë¯¸ ì¹˜í™˜ëœ í…ìŠ¤íŠ¸ì˜ ìœ„ì¹˜ë¥¼ ì €ì¥

    for full_name, variants in ABBREVIATION_GROUPS.items():
        for variant in sorted(variants, key=lambda x: -len(x)):
            # ë³€í˜•ì–´ê°€ ì¡°ì‚¬ì™€ í•¨ê»˜ ë¶™ì€ í˜•íƒœë¡œ ëë‚  ë•Œë„ ë§¤ì¹­
            pattern = re.compile(rf'(?<!\w)({re.escape(variant)}){KOREAN_PARTICLE_PATTERN}')

            def replacer(m):
                start, end = m.start(1), m.end(1)  # variantë§Œí¼ì˜ ë²”ìœ„ë¡œ ë¹„êµ
                for r_start, r_end in replaced_ranges:
                    if not (end <= r_start or start >= r_end):
                        return m.group(0)  # ê¸°ì¡´ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë°˜í™˜

                replaced_ranges.append((start, start + len(full_name)))
                if full_name not in used_majors:
                    used_majors.append(full_name)
                return full_name + (m.group(2) or "")
            query = pattern.sub(replacer, query)
    return query

def extract_date_key_from_query(query: str) -> str | None:
    """
    queryì— DATE_GROUPSì˜ value ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ í•´ë‹¹ keyë¥¼ ë°˜í™˜.
    ì—†ìœ¼ë©´ None ë°˜í™˜.
    """
    for key, phrases in DATE_GROUPS.items():
        for phrase in phrases:
            if phrase in query:
                return key
    return None

def extract_relate_query(query, context_docs):
    prompt = (
    "ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµì˜ í•™ì‚¬ ìš”ëŒ ë° ê³µì§€ì‚¬í•­ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤.\n"
    "- ì•„ë˜ì— ì£¼ì–´ì§„ contextëŠ” ê³¼ê±° ê³µì§€ì‚¬í•­ ë° í•™ì‚¬ ìš”ëŒì˜ ì¼ë¶€ì…ë‹ˆë‹¤.\n"
    "- ì‚¬ìš©ìë¡œë¶€í„°ì˜ queryëŠ” ê·¸ ë‹¤ìŒì— ì£¼ì–´ì§‘ë‹ˆë‹¤.\n"
    "- ì´ queryì™€ ê´€ë ¨í•˜ì—¬ ì‚¬ìš©ìê°€ í›„ì†ìœ¼ë¡œ ê¶ê¸ˆí•´í•  ìˆ˜ ìˆëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ 3ê°€ì§€ë¥¼ í•œêµ­ì–´ë¡œ ì œì•ˆí•˜ì„¸ìš”.\n"
    "- ì£¼ì–´ì§„ contextì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ì§ˆë¬¸ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n"
    "- ì§ˆë¬¸ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ë©°, ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.\n\n"
    )

    messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"query: {query}\n context:{context_docs}\n ì¶”ì²œ ì§ˆë¬¸:"}
        ]

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=4096, # ë‹µë³€ í† í° ìˆ˜ ì œí•œ
            temperature = 0.3,
            top_p = 0.9
        )
        return response.choices[0].message['content']
    except Exception as e:
        print(f"âŒ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return "ì¶”ì²œ ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

def auto_linkify(text):
    if not isinstance(text, str):
        return text
    url_pattern = re.compile(r'(https?://[^\s]+)')
    return url_pattern.sub(r'<a href="\1" target="_blank">ë§í¬</a>', text)

def check_log(log, messages):
    if not log:
        return
    recent_conversation = log[-6:]
    print("í˜„ì¬ log ë‚´ìš©:")
    for i, m in enumerate(log):
        print(f"{i}: role={m['role']}, content={m['content']}")
    for i, msg in enumerate(recent_conversation):
        messages.append(msg)

# âœ… ì¹´í…Œê³ ë¦¬ â†’ ì»¬ë ‰ì…˜ ì´ë¦„ ë§¤í•‘
category_to_collection = {
    "1": "collection_course",
    "2": "collection_subjectinfo",
    "3": "collection_notice"
}

def get_categories():
    return {
        "1": "ê³¼ëª©/ì „ê³µ ì´ìˆ˜ ìš”ê±´",
        "2": "ê³¼ëª© ì •ë³´",
        "3": "í•™ì‚¬ ê³µì§€"
}

retrievers = {}
majors_by_collection = {}
collection_data = load_corpus_by_collection()
top_docs_with_meta = None


def initialize_rag():
    print("ğŸ’¬ í•™ì‚¬ìš”ëŒ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì‹œì‘ë¨.")
    global retrievers, majors_by_collection, collection_data
    if not collection_data:
        print("âš ï¸ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. DBë¥¼ ë¨¼ì € ìƒì„±í•˜ê±°ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()
        
    # âœ… ì»¬ë ‰ì…˜ë³„ retriever ì´ˆê¸°í™”
    for col_name, content in collection_data.items():
        retrievers[col_name] = HybridRetriever(
            content["documents"],
            content["metadatas"],
            collection_name=col_name
        )

    # âœ… major ëª©ë¡ë„ í•¨ê»˜ ì €ì¥
    majors_by_collection = {
        col_name: content["majors"]
        for col_name, content in collection_data.items()
        if "majors" in content
    }

def initialize_cat():
    global top_docs_with_meta

    top_docs_with_meta = None


# ê²€ìƒ‰ê¸°ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def get_response_from_retriever(query: str, selected_collection: str, chat_log: list):
    global top_docs_with_meta
    
    if selected_collection not in retrievers:
        return {
            "answer": f"âŒ ì„ íƒí•œ ì»¬ë ‰ì…˜ '{selected_collection}'ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.",
            "questions": []
        }
        exit()

    retriever = retrievers[selected_collection]
    context_text = ""
    answer = ""

    if selected_collection == "collection_notice":
        #ì‹œê¸° ì¶”ì¶œ
        date_keyword = extract_date_key_from_query(query)

        # ê¸°ë³¸ê°’ ì„¤ì •
        if date_keyword is None:
            print("â„¹ï¸ íŠ¹ì • ì‹œê¸° í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. '2025-1' ë¬¸ì„œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            date_keyword = "2025-1"
        else:
            print(f"âœ¨ '{date_keyword}' ê´€ë ¨ ì •ë³´ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

        query = query.strip()+" "+date_keyword+"ì™€ ê´€ë ¨ëœ ê³µì§€ë¥¼ ì°¾ì•„ì¤˜."
        #print(query)

        # âœ… í•´ë‹¹ ì‹œê¸°ì˜ ë¬¸ì„œë§Œ í•„í„°ë§
        all_docs = collection_data[selected_collection]["documents"]

        context_text = "\n\n".join(all_docs)
        answer=generate_answer(query, all_docs, chat_log, cat=3)

    elif selected_collection == "collection_subjectinfo":
        unique_majors = majors_by_collection[selected_collection]
        # 1) ì¶•ì•½ì–´ ê·¸ë£¹ ì¹˜í™˜ ì ìš©
        query = preprocess_query(query)

        # 2) ë³€í™˜ëœ ì§ˆì˜ë¡œ í•™ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
        major_filter_keyword = extract_major_keyword(query, unique_majors,threshold = 80)

        if major_filter_keyword:
            print(f"âœ¨ '{major_filter_keyword}' ê´€ë ¨ ì •ë³´ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            # 3) í•„í„°ë§ í‚¤ì›Œë“œë¥¼ retrieverì— ì „ë‹¬
            top_docs_with_meta = retriever.retrieve(query, top_k_bm25=3, top_k_dpr=3, filter_major=major_filter_keyword,alpha=0.5,cat= 2)
        else:
            if not top_docs_with_meta:
                print("â„¹ï¸ íŠ¹ì • í•™ê³¼ í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
                # 3) í•„í„°ë§ í‚¤ì›Œë“œë¥¼ retrieverì— ì „ë‹¬
                top_docs_with_meta = retriever.retrieve(query, top_k_bm25=3, top_k_dpr=3, filter_major=major_filter_keyword,alpha=0.5,cat= 2)
            elif extract_subject_by_rapidfuzz(query):
                print("â„¹ï¸ íŠ¹ì • ê³¼ëª© í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. contextë¥¼ ìƒˆë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
                top_docs_with_meta = retriever.retrieve(query, top_k_bm25=3, top_k_dpr=3, filter_major=major_filter_keyword,alpha=0.5,cat= 2)
        
        print(f"query: {query}")

        if not top_docs_with_meta:
            print("\nğŸ§  chatbot ì‘ë‹µ:\nê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

        context_text = "\n\n".join([doc for doc, _ in top_docs_with_meta])
        answer = generate_answer(query, top_docs_with_meta, chat_log, cat=2)

        print("\nğŸ“ ì°¸ê³ í•œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:")
        for doc_content, meta in top_docs_with_meta: # ë¬¸ì„œ ë‚´ìš©ë„ í•¨ê»˜ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            # print(f" - (ë‚´ìš© ì¼ë¶€: {doc_content[:50]}...) ë©”íƒ€ë°ì´í„°: {meta}")
            print(f" - ë©”íƒ€ë°ì´í„°: {meta}")

    else:
        unique_majors = majors_by_collection[selected_collection]
        # 1) ì¶•ì•½ì–´ ê·¸ë£¹ ì¹˜í™˜ ì ìš©
        query = preprocess_query(query)

        print(f"query: {query}")

        # 2) ë³€í™˜ëœ ì§ˆì˜ë¡œ í•™ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
        major_filter_keyword = extract_major_keyword(query, unique_majors,threshold = 60)

        #ìˆ˜ì •!!!!!!
        if major_filter_keyword:
            print(f"âœ¨ '{major_filter_keyword}' ê´€ë ¨ ì •ë³´ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            # 3) í•„í„°ë§ í‚¤ì›Œë“œë¥¼ retrieverì— ì „ë‹¬
            top_docs_with_meta = retriever.retrieve(query, top_k_bm25=10, top_k_dpr=3, filter_major=major_filter_keyword,cat=1)
        else:
            if not top_docs_with_meta:
                print("â„¹ï¸ íŠ¹ì • í•™ê³¼ í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            else:
                print("â„¹ï¸ ê°™ì€ ë¬¸ì„œë¥¼ contextë¡œ ì œê³µí•©ë‹ˆë‹¤.")
        #ì—¬ê¸°ê¹Œì§€ ìˆ˜ì •

        if not top_docs_with_meta:
            print("\nğŸ§  chatbot ì‘ë‹µ:\nê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•˜ê±°ë‚˜ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")

        context_text = "\n\n".join([doc for doc, _ in top_docs_with_meta])
        answer = generate_answer(query, top_docs_with_meta, chat_log, cat=1)

        print("\nğŸ“ ì°¸ê³ í•œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:")
        if top_docs_with_meta is None or not top_docs_with_meta:
            print("No documents retrieved.")
        else:
            for doc_content, meta in top_docs_with_meta: 
                print(f" - ë©”íƒ€ë°ì´í„°: {meta}")

    suggested_questions_text = extract_relate_query(query, context_text)
    # ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬ (ìˆ«ì bullet ì œê±° + strip)
    suggested_question_list = [
        line.lstrip("1234567890.â—â€¢- ").strip()
        for line in suggested_questions_text.strip().splitlines()
        if line.strip()
    ]
    re.sub(r'^\d+\.\s*', '', answer, flags=re.MULTILINE)

    return {
        "answer": answer,
        "questions": suggested_question_list
    }