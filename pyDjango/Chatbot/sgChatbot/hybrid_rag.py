import os
import sys
import django
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'Chatbot.settings')
django.setup()
from django.conf import settings

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
CHROMA_STORE_PATH = os.path.join(BASE_DIR, 'chroma_store')

import openai
import tiktoken
import numpy as np
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from chromadb import PersistentClient
from chromadb.utils.embedding_functions import EmbeddingFunction
import re # ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©ì„ ìœ„í•´ ì¶”ê°€
from rapidfuzz import process
from sgChatbot.dictionary import ABBREVIATION_GROUPS
import math
from sgChatbot.utils import get_user_chat_history
from django.shortcuts import render
from django.contrib.auth.models import User
from django.contrib.auth.decorators import login_required

# âœ… API í‚¤
openai.api_key = settings.OPENAI_API_KEY

# âœ… í•œêµ­ì–´ ì„ë² ë”© í•¨ìˆ˜
class KoSimCSEEmbedding(EmbeddingFunction):
    def __init__(self):
        self.model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
    def __call__(self, input):
        return self.model.encode(input).tolist()

# âœ… ChromaDB ì´ˆê¸°í™” ë° ë¬¸ì„œ+ë©”íƒ€ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
def load_corpus():
    client = PersistentClient(path=CHROMA_STORE_PATH)
    embedding_fn = KoSimCSEEmbedding()
    collections_info = client.list_collections() # get_collection ëŒ€ì‹  list_collections ì‚¬ìš©
    print(f"ì´ {len(collections_info)}ê°œì˜ ì»¬ë ‰ì…˜ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")

    corpus, metadatas_list = [], []
    unique_majors = set() # ê³ ìœ  í•™ê³¼ëª… ì €ì¥ì„ ìœ„í•œ set

    for col_info in collections_info:
        collection = client.get_collection(name=col_info.name, embedding_function=embedding_fn)
        data = collection.get(include=["documents", "metadatas"])
        corpus.extend(data["documents"])
        metadatas_list.extend(data["metadatas"])
        for meta in data["metadatas"]:
            if meta and 'major' in meta: # ë©”íƒ€ë°ì´í„° ë° 'major' í‚¤ ì¡´ì¬ í™•ì¸
                unique_majors.add(meta['major'])

    return corpus, metadatas_list, list(unique_majors) # ê³ ìœ  í•™ê³¼ëª… ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

def load_corpus_by_collection():
    """
    ChromaDBì˜ ëª¨ë“  ì»¬ë ‰ì…˜ì„ ìˆœíšŒí•˜ì—¬,
    ê° ì»¬ë ‰ì…˜ ì´ë¦„ì„ keyë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•´ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        dict[str, dict]: {
            collection_name: {
                "documents": [...],
                "metadatas": [...],
                "majors": [...]
            },
            ...
        }
    """
    client = PersistentClient(path="./chroma_store")
    embedding_fn = KoSimCSEEmbedding()
    collections_info = client.list_collections()

    print(f"ì´ {len(collections_info)}ê°œì˜ ì»¬ë ‰ì…˜ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")

    result = {}

    for col_info in collections_info:
        collection_name = col_info.name
        collection = client.get_collection(name=collection_name, embedding_function=embedding_fn)
        data = collection.get(include=["documents", "metadatas"])

        documents = data["documents"]
        metadatas = data["metadatas"]
        majors = list({meta["major"] for meta in metadatas if meta and "major" in meta})

        result[collection_name] = {
            "documents": documents,
            "metadatas": metadatas,
            "majors": majors
        }

        print(f"âœ… '{collection_name}' ì»¬ë ‰ì…˜ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ. ë¬¸ì„œ ìˆ˜: {len(documents)}")

    return result

def count_total_tokens(messages, model="gpt-4o"):
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")

    total_tokens = 0
    for message in messages:
        total_tokens += 4
        for key, value in message.items():
            total_tokens += len(encoding.encode(value))
    total_tokens += 2
    return total_tokens

# âœ… ì‚¬ìš©ì ì§ˆì˜ì—ì„œ í•™ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_major_keyword(query, majors_list):
    """
    ì‚¬ìš©ì ì§ˆì˜ì—ì„œ ì–¸ê¸‰ëœ í•™ê³¼ í‚¤ì›Œë“œë¥¼ majors_list (DBì— ì €ì¥ëœ ì‹¤ì œ í•™ê³¼ëª…) ê¸°ì¤€ìœ¼ë¡œ ìœ ì‚¬ ë¬¸ìì—´ ë§¤ì¹­í•˜ì—¬ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë„ì–´ì“°ê¸°, ì˜¤íƒ€, ì¶•ì•½ì–´ ì°¨ì´ ë“±ìœ¼ë¡œ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì•„ë„ ê°€ì¥ ìœ ì‚¬í•œ í•™ê³¼ëª…ì„ ì°¾ì•„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ì‚¬ìš©ì ì§ˆì˜ ì •ê·œí™”: ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜
    normalized_query = query.replace(" ", "").lower()

    # majors_listì˜ í•™ê³¼ëª…ë„ ì •ê·œí™” (ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°, ì†Œë¬¸ì ë³€í™˜)
    candidates = [m.replace("_", "").lower() for m in majors_list]

    # rapidfuzzì˜ extractOneìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ í•™ê³¼ëª…ê³¼ ìœ ì‚¬ë„ ë°˜í™˜
    best_match = process.extractOne(normalized_query, candidates)

    # ìœ ì‚¬ë„ ê¸°ì¤€ ì„¤ì • (ì˜ˆ: 80 ì´ìƒì¼ ë•Œë§Œ ë§¤ì¹­ ì¸ì •)
    '''if best_match and best_match[1] > 80:
        idx = candidates.index(best_match[0])
        return majors_list[idx]'''
    
    # âœ… ìœ ì‚¬í•œ í•­ëª© ì—¬ëŸ¬ ê°œ ì¶”ì¶œ
    matches = process.extract(normalized_query, candidates, limit=3)
    # âœ… ìœ ì‚¬ë„ ê¸°ì¤€ í†µê³¼í•œ í•™ê³¼ë§Œ ë°˜í™˜
    result = []
    for match_str, score, _ in matches:
        idx = candidates.index(match_str)
        print(majors_list[idx])
        if score >= 80:
            idx = candidates.index(match_str)
            matched_major = majors_list[idx]
            result.append(matched_major)
            print(f"âœ… ìœ ì‚¬ë„ {score} â†’ ë§¤ì¹­ëœ í•™ê³¼: {matched_major}")
        else:
            print(f"âŒ ìœ ì‚¬ë„ {score} â†’ ë¬´ì‹œë¨")
        
    return result  # ìµœëŒ€ top_kê°œì˜ í•™ê³¼ëª… ë°˜í™˜


    # "ê³µì§€", "ì•ˆë‚´", "ì¼ì •" ë“± ì¼ë°˜ì ì¸ í‚¤ì›Œë“œë„ ì—¬ê¸°ì„œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì˜ˆë¥¼ ë“¤ì–´ 'ê³µì§€'ë¼ëŠ” ë‹¨ì–´ê°€ ìˆìœ¼ë©´, ë¬¸ì„œ ìœ í˜•ì„ 'ê³µì§€'ë¡œ í•„í„°ë§í•˜ë„ë¡ ì„¤ì •
    # if "ê³µì§€" in query:
    # return {"type": "notice"} # ì´ëŸ° ì‹ìœ¼ë¡œ ë‹¤ë¥¸ í•„í„°ë§ ê¸°ì¤€ë„ ì¶”ê°€ ê°€ëŠ¥

    # ë§¤ì¹­ë˜ëŠ” í•™ê³¼ëª…ì´ ì—†ìœ¼ë©´ None ë°˜í™˜
    return None

# âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
class HybridRetriever:
    def __init__(self, corpus_all, metadatas_all,collection_name):
        # ì´ˆê¸°í™” ì‹œì ì—ëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ë³´ê´€
        self.corpus_all = corpus_all
        self.metadatas_all = metadatas_all
        self.encoder = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
        self.collection_name = collection_name
        # ì „ì²´ ë¬¸ì„œì— ëŒ€í•œ ì„ë² ë”©ì„ ë¯¸ë¦¬ ê³„ì‚°í•´ë‘˜ ìˆ˜ ìˆìœ¼ë‚˜, í•„í„°ë§ ì‹œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³ ë ¤
        # self.dense_embeddings_all = self.encoder.encode(self.corpus_all) # í•„ìš” ì‹œ í™œì„±í™”

    def retrieve(self, query, top_k_bm25=10, top_k_dpr=3, filter_major=None):
        # ì‹¤ì œ ê²€ìƒ‰ ëŒ€ìƒì´ ë  ì½”í¼ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°
        current_corpus = self.corpus_all
        current_metadatas = self.metadatas_all

        # í•„í„°ë§í•  í•™ê³¼ê°€ ì§€ì •ëœ ê²½ìš°
        if filter_major:
            print(f"ğŸ” '{filter_major}' í•™ê³¼ ê´€ë ¨ ë¬¸ì„œë¡œ í•„í„°ë§ ì¤‘...")
            filtered_indices = [
                i for i, meta in enumerate(self.metadatas_all)
                if meta and meta.get('major') in filter_major #filter_majorë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°”ê¿ˆ
            ]
            if not filtered_indices:
                print(f"âš ï¸ '{filter_major}' í•™ê³¼ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            else:
                current_corpus = [self.corpus_all[i] for i in filtered_indices]
                current_metadatas = [self.metadatas_all[i] for i in filtered_indices]
                print(f"ğŸ” í•„í„°ë§ ê²°ê³¼: ì´ {len(current_corpus)}ê°œì˜ ë¬¸ì„œë¡œ ì œí•œë¨.")


        if not current_corpus: # í•„í„°ë§ í›„ ë¬¸ì„œê°€ ì—†ì„ ê²½ìš°
             print("âš ï¸ ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
             return []

        # BM25 ê³„ì‚° (í•„í„°ë§ëœ ì½”í¼ìŠ¤ ë˜ëŠ” ì „ì²´ ì½”í¼ìŠ¤ ëŒ€ìƒ)
        tokenized_corpus = [doc.split() for doc in current_corpus]
        
        bm25 = BM25Okapi(tokenized_corpus,b=0.25)
        tokenized_query = query.split()
        bm25_scores = bm25.get_scores(tokenized_query)

        # BM25 ê²°ê³¼ê°€ top_k_bm25ë³´ë‹¤ ì ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„
        num_bm25_candidates = min(top_k_bm25, len(current_corpus))
        bm25_indices_in_current = np.argsort(bm25_scores)[::-1][:num_bm25_candidates]
        bm25_candidates_docs = [current_corpus[i] for i in bm25_indices_in_current]
        bm25_candidates_meta = [current_metadatas[i] for i in bm25_indices_in_current]

        # -------------------------------
        # 2ï¸âƒ£ ChromaDB ë²¡í„° ë¶ˆëŸ¬ì™€ ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°
        # -------------------------------
        
        embedding_fn = KoSimCSEEmbedding()
        client = PersistentClient(path="./chroma_store")
        collection = client.get_collection(name=self.collection_name, embedding_function=embedding_fn)

        # BM25 í›„ë³´ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œ ID ìƒì„±
        bm25_candidate_ids = []
        for meta in bm25_candidates_meta:
            file = meta.get("source_file", "")
            university = meta.get("university", "")
            major = meta.get("major", "")
            bm25_candidate_ids.append(f"{file}_{university}_{major}")

        # í•´ë‹¹ IDë“¤ì˜ ì„ë² ë”© ë¶ˆëŸ¬ì˜¤ê¸°
        retrieved = collection.get(ids=bm25_candidate_ids, include=["embeddings"])
        candidate_embeddings = retrieved["embeddings"]

        # ì§ˆì˜ ì„ë² ë”© ìƒì„±
        query_embedding = self.encoder.encode([query])
        similarities = cosine_similarity(query_embedding, candidate_embeddings)[0]

        # Top-K DPR ìœ ì‚¬ë„ ì •ë ¬
        num_dpr_candidates = min(top_k_dpr, len(bm25_candidates_docs))
        top_indices_in_bm25 = np.argsort(similarities)[::-1][:num_dpr_candidates]

        # âœ… í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        alpha = 0.5
        bm25_top_scores = np.array([bm25_scores[idx] for idx in bm25_indices_in_current])
        bm25_norm = (bm25_top_scores - np.min(bm25_top_scores)) / (np.max(bm25_top_scores) - np.min(bm25_top_scores) + 1e-8)
        dpr_norm = (similarities - np.min(similarities)) / (np.max(similarities) - np.min(similarities) + 1e-8)

        hybrid_scores = alpha * bm25_norm + (1 - alpha) * dpr_norm
        top_indices = np.argsort(hybrid_scores)[::-1][:top_k_dpr]

        sorted_indices = np.argsort(hybrid_scores)[::-1]  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ ì¸ë±ìŠ¤

        for rank, i in enumerate(sorted_indices):
            raw_bm25_score = bm25_top_scores[i]
            raw_dpr_score = similarities[i]
    
            print(f"  [{rank}] í†µí•©ì ìˆ˜: {hybrid_scores[i]:.4f} | "
                f"BM25(norm): {bm25_norm[i]:.4f} | BM25(raw): {raw_bm25_score:.4f} | "
                f"DPR(norm): {dpr_norm[i]:.4f} | DPR(raw): {raw_dpr_score:.4f} | "
                f"ë©”íƒ€: {bm25_candidates_meta[i]}")


        final_results = [
            (bm25_candidates_docs[i], bm25_candidates_meta[i]) for i in sorted_indices[:top_k_dpr]
        ]


        return final_results

# âœ… GPT ì‘ë‹µ ìƒì„±ê¸°
def generate_answer(query, context_docs, request):
    if not context_docs: # ì°¸ê³  ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ì£¼ì„¸ìš”."

    context = "\n\n".join([
        f"[{meta.get('university', 'Unknown University')} - {meta.get('major', 'Unknown Major')} - {meta.get('source_file', 'Unknown Source')}]\n{doc}"
        for doc, meta in context_docs
    ])


    prompt = (
        "ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµì˜ í•™ì‚¬ ìš”ëŒê³¼ ê³µì§€ì‚¬í•­ì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n"
        "ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë”ë¼ë„ ê´€ë ¨ í•™ê³¼ ë˜ëŠ” ê·œì • ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
        "ê°™ì€ ê³¼ëª©ì— ëŒ€í•œ ì„¤ëª…ì´ ì—¬ëŸ¬ í•™ê³¼ì˜ ë¬¸ì„œì— ë‚˜ë‰˜ì–´ ìˆì„ ê²½ìš°, ì„¤ëª…ì´ ìˆëŠ” í•™ê³¼ë“¤ì—ì„œì˜ ì„¤ëª…ì„ ëª¨ë‘ ë„£ì–´ ì£¼ì„¸ìš”.\n"
        "ì¤‘ë³µëœ ë‚´ìš©ì€ ê°ê°ì˜ ë¬¸ë§¥ì—ì„œ í•„ìš”í•œ ê²½ìš° ë°˜ë³µí•´ë„ ë©ë‹ˆë‹¤.\n"
        "ê° í•™ê³¼ë³„ì˜ ì„¤ëª…ì€ ë¶„ë¦¬ëœ ë¬¸ë‹¨ìœ¼ë¡œ ì¶œë ¥í•´ì£¼ì„¸ìš”.\n"
        "ì œê³µëœ contextì—ì„œ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ ì°¾ì„ ìˆ˜ ì—†ë‹¤ê³  ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•´ì£¼ì„¸ìš”.\n"
    )

    # âœ… ìµœê·¼ ëŒ€í™” ì´ë ¥ 3ê°œ ë¶ˆëŸ¬ì˜¤ê¸° (user_idê°€ ì œê³µëœ ê²½ìš°)
    recent_history = []
    
    if request is not None and request.user.is_authenticated:
        user = request.user
        user_id = user.id

        history = get_user_chat_history(user_id)
        if history:
            last_3 = history[-3:]
            for h in last_3:
                if h["type"] == "user":
                    recent_history.append({"role": "user", "content": h["text"]})
                elif h["type"] == "bot":
                    recent_history.append({"role": "assistant", "content": h["text"]})

    messages = [{"role": "system", "content": prompt}]
    messages.extend(recent_history)  # ğŸ” ìµœê·¼ ì±„íŒ… ë‚´ì—­ ì‚½ì…
    messages.append({"role": "user", "content": f"context:\n{context}\n\nì§ˆë¬¸: {query}\në‹µë³€:"}) # ìµœì¢… ì§ˆì˜

    total_tokens = count_total_tokens(messages, model="gpt-4o")
    max_tokens_model = 128000 # ëª¨ë¸ì˜ ìµœëŒ€ í† í° (gpt-4o ê¸°ì¤€)
    max_response_tokens = 4096 # ë‹µë³€ìœ¼ë¡œ ë°›ê³ ì í•˜ëŠ” ìµœëŒ€ í† í° ìˆ˜

    # í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ ê²½ìš°, ëª¨ë¸ì˜ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ìë¥´ê±°ë‚˜,
    # ë‹µë³€ ìƒì„± í† í° ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ ì…ë ¥ í† í°ì„ ì¡°ì ˆí•´ì•¼ í•¨.
    # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ ê²½ê³ ë§Œ ì¶œë ¥
    if total_tokens > max_tokens_model - max_response_tokens : # ëª¨ë¸ í•œê³„ - ì‘ë‹µ í† í° = í”„ë¡¬í”„íŠ¸ ìµœëŒ€
        print(f"âš ï¸ ì „ì²´ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜ê°€ {total_tokens}ê°œë¡œ ëª¨ë¸ ì œí•œ({max_tokens_model})ì„ ì´ˆê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë¶€ ì»¨í…ìŠ¤íŠ¸ê°€ ì˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        # ì‹¤ì œë¡œëŠ” contextë¥¼ ì¤„ì´ëŠ” ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.
    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=1000, # ë‹µë³€ í† í° ìˆ˜ ì œí•œ
            temperature = 0.3,
            top_p = 0.9
        )
        return response.choices[0].message['content'] # ìˆ˜ì •: .message.content -> .message['content']
    except openai.error.InvalidRequestError as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # í† í° ì´ˆê³¼ ì—ëŸ¬ì˜ ê²½ìš°, ì—¬ê¸°ì„œ contextë¥¼ ì¤„ì—¬ì„œ ì¬ì‹œë„í•˜ëŠ” ë¡œì§ì„ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        if "maximum context length" in str(e):
            return "ì§ˆë¬¸ê³¼ ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” ì§§ê²Œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜, í•„í„°ë§ì„ í†µí•´ ë¬¸ì„œ ë²”ìœ„ë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”."
        return "ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    except Exception as e: # ë‹¤ë¥¸ ì˜ˆì™¸ ì²˜ë¦¬
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


KOREAN_PARTICLE_PATTERN = r'(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ìœ¼ë¡œ|ë¡œ|ë„|ë§Œ|ê¹Œì§€|ë¶€í„°|ì¡°ì°¨|ì¸ë°|ê³ |ì™€|ê³¼|ë§ˆì €|ì²˜ëŸ¼|ê»˜ì„œ|ë°–ì—|ì´ë©°|ì´ê³ |ì´ë‚˜|ë¼ë„|ë¼ê³ |ë¼ëŠ”|ë“ ì§€|ë§Œí¼)?'

def preprocess_query(query):
    used_majors = []
    replaced_ranges = []  # ì´ë¯¸ ì¹˜í™˜ëœ í…ìŠ¤íŠ¸ì˜ ìœ„ì¹˜ë¥¼ ì €ì¥

    for full_name, variants in ABBREVIATION_GROUPS.items():
        for variant in sorted(variants, key=lambda x: -len(x)):
            pattern = re.compile(rf'({re.escape(variant)}){KOREAN_PARTICLE_PATTERN}')
            def replacer(m):
                start, end = m.start(), m.end()
                # ê¸°ì¡´ ì¹˜í™˜ëœ ì˜ì—­ê³¼ ê²¹ì¹˜ë©´ ë¬´ì‹œ
                for r_start, r_end in replaced_ranges:
                    if not (end <= r_start or start >= r_end):
                        return m.group(0)  # ê¸°ì¡´ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë°˜í™˜

                replaced_ranges.append((start, start + len(full_name)))
                if full_name not in used_majors:
                    used_majors.append(full_name)
                return full_name + (m.group(2) or "")
            query = pattern.sub(replacer, query)
    return query, used_majors

# âœ… ì¹´í…Œê³ ë¦¬ â†’ ì»¬ë ‰ì…˜ ì´ë¦„ ë§¤í•‘
category_to_collection = {
    "1": "collection_course",
    "2": "collection_subjectinfo"
}

def get_categories():
    return {
        "1": "ê³¼ëª©/ì „ê³µ ì´ìˆ˜ ìš”ê±´",
        "2": "ê³¼ëª© ì •ë³´"
    }

retrievers = {}
majors_by_collection = {}

def initialize_rag():
    global retrievers, majors_by_collection
    collection_data = load_corpus_by_collection()
    if not collection_data:
        print("âš ï¸ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. DBë¥¼ ë¨¼ì € ìƒì„±í•˜ê±°ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()
        
    # âœ… ì»¬ë ‰ì…˜ë³„ retriever ì´ˆê¸°í™”
    retrievers = {
        col_name: HybridRetriever(
            content["documents"],
            content["metadatas"],
            collection_name=col_name
        ) for col_name, content in collection_data.items()
    }    

    # âœ… major ëª©ë¡ë„ í•¨ê»˜ ì €ì¥
    majors_by_collection = {
        col_name: content["majors"] for col_name, content in collection_data.items()
    }

# ê²€ìƒ‰ê¸°ì—ì„œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
def get_response_from_retriever(query: str, selected_collection: str) -> str:
    if selected_collection not in retrievers:
        return f"âŒ ì„ íƒí•œ ì»¬ë ‰ì…˜ '{selected_collection}'ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        exit()

    retriever = retrievers[selected_collection]
    top_docs_with_meta = retriever.retrieve(query, top_k_bm25=10, top_k_dpr=3)

    if not top_docs_with_meta:
        return "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."

    answer = generate_answer(query, top_docs_with_meta, request=None)
    return f"{answer}"

initialize_rag()

# âœ… ë©”ì¸ ì‹¤í–‰ ë£¨í”„
if __name__ == "__main__":
    print("ğŸ’¬ í•™ì‚¬ìš”ëŒ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì‹œì‘ë¨.")
    categoris = get_categories()
    print("ì§ˆë¬¸ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
    cat = input("\n1. ê³¼ëª©/ì „ê³µ ì´ìˆ˜ ìš”ê±´ 2. ê³¼ëª© ì •ë³´\n-> ")
    
    selected_collection = category_to_collection[cat]

    while True:
        query = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œë¥¼ ì›í•˜ë©´ exitì„, category ë³€ê²½ì„ ì›í•˜ë©´ catì„ ì…ë ¥í•´ì£¼ì„¸ìš”.): ")
        if query.lower().strip() == "exit":
            print("ğŸš«ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")
            break
        elif query.lower().strip() == "cat":
            print("ì§ˆë¬¸ì˜ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
            cat = input("\n1. ê³¼ëª©/ì „ê³µ ì´ìˆ˜ ìš”ê±´ 2. ê³¼ëª© ì •ë³´\n-> ")

            if cat not in category_to_collection:
                print("âš ï¸ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. 1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
                continue
    
            selected_collection = category_to_collection[cat]

            if selected_collection not in retrievers:
                print(f"âŒ ì„ íƒí•œ ì»¬ë ‰ì…˜ '{selected_collection}'ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                break

            retriever = retrievers[selected_collection]
            unique_majors = majors_by_collection[selected_collection]

            query = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œë¥¼ ì›í•˜ë©´ exitì„, category ë³€ê²½ì„ ì›í•˜ë©´ catì„ ì…ë ¥í•´ì£¼ì„¸ìš”.): ")

        # 1) ì¶•ì•½ì–´ ê·¸ë£¹ ì¹˜í™˜ ì ìš©
        query,used_major = preprocess_query(query)

        print(f"query: {query}")
        print(f"major: {used_major}")

        # 2) ë³€í™˜ëœ ì§ˆì˜ë¡œ í•™ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
        major_filter_keyword = extract_major_keyword(query, unique_majors)

        if major_filter_keyword:
            print(f"âœ¨ '{major_filter_keyword}' ê´€ë ¨ ì •ë³´ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        else:
            print("â„¹ï¸ íŠ¹ì • í•™ê³¼ í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        

        # 3) í•„í„°ë§ í‚¤ì›Œë“œë¥¼ retrieverì— ì „ë‹¬
        top_docs_with_meta = retriever.retrieve(query, top_k_bm25=10, top_k_dpr=3, filter_major=major_filter_keyword)

        if not top_docs_with_meta:
            print("\nğŸ§  chatbot ì‘ë‹µ:\nê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            continue # ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ

        answer = generate_answer(query, top_docs_with_meta, request=None)
        print(f"\nğŸ§  chatbot ì‘ë‹µ:\n{answer}")

        print("\nğŸ“ ì°¸ê³ í•œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:")
        for doc_content, meta in top_docs_with_meta: # ë¬¸ì„œ ë‚´ìš©ë„ í•¨ê»˜ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            # print(f" - (ë‚´ìš© ì¼ë¶€: {doc_content[:50]}...) ë©”íƒ€ë°ì´í„°: {meta}")
            print(f" - ë©”íƒ€ë°ì´í„°: {meta}")