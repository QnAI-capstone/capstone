import json
from chromadb import PersistentClient
from chromadb.utils.embedding_functions import EmbeddingFunction
from sentence_transformers import SentenceTransformer
from sgChatbot.flat_json import flatten_json_to_text
from sgChatbot.chunk_split import group_by_course_blocks
from sgChatbot.flatten_notice import flat_notice, merge_by_index
import os

# collection ì´ë¦„: collection_course, collection_subjectinfo, collection_notice

# âœ… í•œêµ­ì–´ ì„ë² ë”© í•¨ìˆ˜ (KoSimCSE)
class KoSimCSEEmbedding(EmbeddingFunction):
    def __init__(self):
        self.model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
    def __call__(self, input):
        return self.model.encode(input).tolist()

# âœ… ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = PersistentClient(path="./chroma_store")
embedding_fn = KoSimCSEEmbedding()

# âœ… JSON íŒŒì¼ì´ ë“¤ì–´ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
json_dir_list = ["course", "subjectinfo", "notice"] # JSON íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ (í•„ìš” ì‹œ ë³€ê²½)

# âœ… ë””ë ‰í† ë¦¬ë³„ë¡œ ì»¬ë ‰ì…˜ ìƒì„± ë° íŒŒì¼ ì²˜ë¦¬
for json_dir in json_dir_list:
    # ë””ë ‰í† ë¦¬ ì´ë¦„ì„ ì»¬ë ‰ì…˜ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
    collection_name = f"collection_{json_dir}"
    try:
        collection = client.get_collection(collection_name)
        print(f"âš ï¸ ì»¬ë ‰ì…˜ '{collection_name}'ì€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ì»¬ë ‰ì…˜ì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"ì»¬ë ‰ì…˜ '{collection_name}'ì„ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        collection = client.create_collection(
            name=collection_name,
            embedding_function=embedding_fn
        )
        file_list = [f for f in os.listdir(json_dir) if f.endswith(".json")]

        # âœ… ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  JSON íŒŒì¼ì„ ì²˜ë¦¬
        for file_name in file_list:
            file_path = os.path.join(json_dir, file_name)
            with open(file_path, "r", encoding="utf-8") as f:
                raw = json.load(f)

            if collection_name == "collection_notice":
                # âœ… flatten í›„ ì¸ë±ìŠ¤ ê¸°ì¤€ í•œ ì¤„ë¡œ merge
                flat = flat_notice(raw)
                text = merge_by_index(flat)

                # âœ… metadata êµ¬ì„±
                date_part = file_name.rsplit("_", 1)[-1].replace(".json", "")
                metadata = {
                    "source_file": file_name,
                    "date": date_part
                }

                # âœ… Chroma DBì— ì¶”ê°€
                collection.add(
                    documents=[text],
                    metadatas=[metadata],
                    ids=[file_name.replace(".json", "")]
                )
                continue

            # "2025 ì„œê°•ëŒ€í•™êµ ìš”ëŒ" ìµœìƒìœ„ keyë¡œ ì ‘ê·¼
            base_key = "2025 ì„œê°•ëŒ€í•™êµ ìš”ëŒ"
            if base_key not in raw:
                print(f"âš ï¸ '{base_key}' í‚¤ê°€ íŒŒì¼ {file_name}ì— ì—†ìŠµë‹ˆë‹¤.")
                continue

            base_data = raw[base_key]

            # ëŒ€í•™ëª…ë³„ ìˆœíšŒ
            for university_name, majors_data in base_data.items():
                safe_university = university_name.replace(" ", "_").replace("(", "").replace(")", "")

                # í•™ê³¼ë³„ ìˆœíšŒ
                for major, data in majors_data.items():
                    safe_major = major.replace(" ", "_").replace("(", "").replace(")", "")
                    doc_id = f"{file_name}_{safe_university}_{safe_major}"

                    print(f"ğŸ“Œ {safe_university} - {safe_major} ì €ì¥ ì¤‘.")

                    if collection_name == "collection_course":
                        text = flatten_json_to_text(data)

                        metadata = {
                            "university": safe_university,
                            "major": safe_major,
                            "source_file": file_name,
                        }

                        collection.add(
                            documents=[text],
                            metadatas=[metadata],
                            ids=[doc_id]
                        )

                    elif collection_name == "collection_subjectinfo":
                        chunks = group_by_course_blocks(flatten_json_to_text(data))

                        for chunk_idx, chunk in enumerate(chunks):
                            chunk_id = f"{file_name}_{safe_university}_{safe_major}_{chunk_idx}"

                            metadata = {
                                "university": safe_university,
                                "major": safe_major,
                                "source_file": file_name,
                                "section": chunk_idx,  # or add section name if available
                                "id":chunk_id
                            }

                            collection.add(
                                documents=[chunk],
                                metadatas=[metadata],
                                ids=[chunk_id]
                            )

                    print(f"ğŸ“Œ {safe_university} - {safe_major} ë°ì´í„° ì €ì¥ ì™„ë£Œ. í˜„ì¬ ì»¬ë ‰ì…˜ ë°ì´í„° ìˆ˜: {collection.count()}")
                    print(f"    ì €ì¥ëœ ë©”íƒ€ë°ì´í„°: {metadata}")

        print("âœ… ì „ì²´ íŒŒì¼ ì €ì¥ ì™„ë£Œ.")

        # ì»¬ë ‰ì…˜ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        collections = client.list_collections()

        # ìƒì„±ëœ ì»¬ë ‰ì…˜ ë° metadata ì¶œë ¥
        print("í˜„ì¬ ìƒì„±ëœ ì»¬ë ‰ì…˜ë“¤ ë° ë©”íƒ€ë°ì´í„°:")

        for col_info in collections:
            print(f"\nì»¬ë ‰ì…˜ ì´ë¦„: {col_info.name}")
            collection = client.get_collection(name=col_info.name)
            data = collection.get(include=["metadatas"], limit=5)  # limitì€ ì¶œë ¥í•  ë¬¸ì„œ ìˆ˜ ì œí•œ
            metadatas = data.get("metadatas", [])

            if not metadatas:
                print(" - ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue

            print(" - ì¼ë¶€ ë©”íƒ€ë°ì´í„°:")
            for meta in metadatas:
                print(f"    {meta}")