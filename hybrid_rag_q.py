from config import OPENAI_API_KEY
import openai
import tiktoken
import numpy as np
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from chromadb import PersistentClient
from chromadb.utils.embedding_functions import EmbeddingFunction
import re # ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©ì„ ìœ„í•´ ì¶”ê°€
from rapidfuzz import process
from dictionary import ABBREVIATION_GROUPS,DATE_GROUPS
import math
from ex_sub import extract_subject_by_rapidfuzz
from scipy.stats import rankdata

# âœ… API í‚¤
openai.api_key = OPENAI_API_KEY

# âœ… í•œêµ­ì–´ ì„ë² ë”© í•¨ìˆ˜
class KoSimCSEEmbedding(EmbeddingFunction):
    def __init__(self):
        self.model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
    def __call__(self, input):
        return self.model.encode(input).tolist()

#notice -> ìˆ˜ì •
def load_corpus_by_collection():
    """
    ChromaDBì˜ ëª¨ë“  ì»¬ë ‰ì…˜ì„ ìˆœíšŒí•˜ì—¬,
    ê° ì»¬ë ‰ì…˜ ì´ë¦„ì„ keyë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•´ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        dict[str, dict]: {
            collection_name: {
                "documents": [...],
                "metadatas": [...],
                "majors": [...]
            },
            ...
        }
    """
    client = PersistentClient(path="./chroma_store")
    embedding_fn = KoSimCSEEmbedding()
    collections_info = client.list_collections()

    print(f"ì´ {len(collections_info)}ê°œì˜ ì»¬ë ‰ì…˜ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")

    result = {}

    for col_info in collections_info:
        collection_name = col_info.name
        collection = client.get_collection(name=collection_name, embedding_function=embedding_fn)
        data = collection.get(include=["documents", "metadatas"])

        documents = data["documents"]
        metadatas = data["metadatas"]

        #ê³µì§€ ë°ì´í„° ì²˜ë¦¬
        if collection_name == "collection_notice":
            dates = list({meta.get("date") for meta in metadatas if meta and "date" in meta})
            result[collection_name] = {
                "documents": documents,
                "metadatas": metadatas,
                "dates": dates
            }
            print(f"ğŸ“Œ 'notice_collection' ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ. ë¬¸ì„œ ìˆ˜: {len(documents)}")

        
        #ê³¼ëª© ì´ìˆ˜, ê³¼ëª© ì„¤ëª… ë°ì´í„° ì²˜ë¦¬
        else:
            majors = list({meta["major"] for meta in metadatas if meta and "major" in meta})

            result[collection_name] = {
                "documents": documents,
                "metadatas": metadatas,
                "majors": majors
            }

            print(f"âœ… '{collection_name}' ì»¬ë ‰ì…˜ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ. ë¬¸ì„œ ìˆ˜: {len(documents)}")

    return result

def count_total_tokens(messages, model="gpt-4o"):
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")

    total_tokens = 0
    for message in messages:
        total_tokens += 4
        for key, value in message.items():
            total_tokens += len(encoding.encode(value))
    total_tokens += 2
    return total_tokens

# âœ… ì‚¬ìš©ì ì§ˆì˜ì—ì„œ í•™ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_major_keyword(query, majors_list,threshold=70):
    """
    ì‚¬ìš©ì ì§ˆì˜ì—ì„œ ì–¸ê¸‰ëœ í•™ê³¼ í‚¤ì›Œë“œë¥¼ majors_list (DBì— ì €ì¥ëœ ì‹¤ì œ í•™ê³¼ëª…) ê¸°ì¤€ìœ¼ë¡œ ìœ ì‚¬ ë¬¸ìì—´ ë§¤ì¹­í•˜ì—¬ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë„ì–´ì“°ê¸°, ì˜¤íƒ€, ì¶•ì•½ì–´ ì°¨ì´ ë“±ìœ¼ë¡œ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì•„ë„ ê°€ì¥ ìœ ì‚¬í•œ í•™ê³¼ëª…ì„ ì°¾ì•„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ì‚¬ìš©ì ì§ˆì˜ ì •ê·œí™”: ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜
    normalized_query = query.replace(" ", "").lower()

    # majors_listì˜ í•™ê³¼ëª…ë„ ì •ê·œí™” (ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°, ì†Œë¬¸ì ë³€í™˜)
    candidates = [m.replace("_", "").lower() for m in majors_list]

    # rapidfuzzì˜ extractOneìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ í•™ê³¼ëª…ê³¼ ìœ ì‚¬ë„ ë°˜í™˜
    best_match = process.extractOne(normalized_query, candidates)
    
    # âœ… ìœ ì‚¬í•œ í•­ëª© ì—¬ëŸ¬ ê°œ ì¶”ì¶œ
    matches = process.extract(normalized_query, candidates, limit=2)
    # âœ… ìœ ì‚¬ë„ ê¸°ì¤€ í†µê³¼í•œ í•™ê³¼ë§Œ ë°˜í™˜
    result = []
    for match_str, score, _ in matches:
        idx = candidates.index(match_str)
        print(majors_list[idx])
        if score >= threshold:
            idx = candidates.index(match_str)
            matched_major = majors_list[idx]
            result.append(matched_major)
            print(f"âœ… ìœ ì‚¬ë„ {score} â†’ ë§¤ì¹­ëœ í•™ê³¼: {matched_major}")
        else:
            print(f"âŒ ìœ ì‚¬ë„ {score} â†’ ë¬´ì‹œë¨")
        
    return result  # ìµœëŒ€ top_kê°œì˜ í•™ê³¼ëª… ë°˜í™˜



# âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
class HybridRetriever:
    def __init__(self, corpus_all, metadatas_all,collection_name):
        # ì´ˆê¸°í™” ì‹œì ì—ëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ë³´ê´€
        self.corpus_all = corpus_all
        self.metadatas_all = metadatas_all
        self.encoder = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
        self.collection_name = collection_name

    def retrieve(self, query, top_k_bm25=10, top_k_dpr=3, filter_major=None,alpha=0.5,cat=1):
        print("ğŸŸ¢ ë¬¸ì„œë¥¼ retrieve í•¨ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        # ì‹¤ì œ ê²€ìƒ‰ ëŒ€ìƒì´ ë  ì½”í¼ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°
        current_corpus = self.corpus_all
        current_metadatas = self.metadatas_all
        final_results = []
        query_bm25 = query
        flag = 0

        # í•„í„°ë§í•  í•™ê³¼ê°€ ì§€ì •ëœ ê²½ìš°
        if filter_major:
            print(f"ğŸ” '{filter_major}' í•™ê³¼ ê´€ë ¨ ë¬¸ì„œë¡œ í•„í„°ë§ ì¤‘...")
            filtered_indices = [
                i for i, meta in enumerate(self.metadatas_all)
                if meta and meta.get('major') in filter_major #filter_majorë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°”ê¿ˆ
            ]
            

            if not filtered_indices:
                print(f"âš ï¸ '{filter_major}' í•™ê³¼ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            else:
                current_corpus = [self.corpus_all[i] for i in filtered_indices]
                current_metadatas = [self.metadatas_all[i] for i in filtered_indices]
                print(f"ğŸ” í•„í„°ë§ ê²°ê³¼: ì´ {len(current_corpus)}ê°œì˜ ë¬¸ì„œë¡œ ì œí•œë¨.")


        if not current_corpus: # í•„í„°ë§ í›„ ë¬¸ì„œê°€ ì—†ì„ ê²½ìš°
             print("âš ï¸ ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
             return []
        
        if cat == 2:
            sub = extract_subject_by_rapidfuzz(query)
            if sub:
                query_bm25 = query.strip()+" "+sub[0]+" ê³¼ëª©"
        
        print(f"query bm25 : {query_bm25}")

        # BM25 ê³„ì‚° (í•„í„°ë§ëœ ì½”í¼ìŠ¤ ë˜ëŠ” ì „ì²´ ì½”í¼ìŠ¤ ëŒ€ìƒ)
        tokenized_corpus = [doc.split() for doc in current_corpus]
        
        bm25 = BM25Okapi(tokenized_corpus,b=0.25)
        tokenized_query = query_bm25.split()
        bm25_scores = bm25.get_scores(tokenized_query)

        # BM25 ê²°ê³¼ê°€ top_k_bm25ë³´ë‹¤ ì ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„
        num_bm25_candidates = min(top_k_bm25, len(current_corpus))

        bm25_indices_in_current = np.argsort(bm25_scores)[::-1][:num_bm25_candidates]
        print(num_bm25_candidates)
        bm25_candidates_docs = [current_corpus[i] for i in bm25_indices_in_current]
        bm25_candidates_meta = [current_metadatas[i] for i in bm25_indices_in_current]




        # -------------------------------
        # 2ï¸âƒ£ cat = 2ì¸ ê²½ìš°ì—ë§Œ ChromaDB ë²¡í„° ë¶ˆëŸ¬ì™€ ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°
        # -------------------------------
        
        if cat == 2:
            embedding_fn = KoSimCSEEmbedding()
            client = PersistentClient(path="./chroma_store")
            collection = client.get_collection(name=self.collection_name, embedding_function=embedding_fn)

            retrieved = collection.get(include=["embeddings", "metadatas", "documents"])
            all_embeddings = retrieved["embeddings"]
            all_metadatas = retrieved["metadatas"]
            all_documents = retrieved["documents"]

            # ì§ˆì˜ ì„ë² ë”©
            query_embedding = self.encoder.encode([query])  # (1, dim)

            # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì „ì²´ ë¬¸ì„œ ëŒ€ìƒ)
            similarities = cosine_similarity(query_embedding, all_embeddings)[0]  # (num_docs,)

            # 4ï¸âƒ£ ìƒìœ„ top_k_dpr ê°œìˆ˜ë§Œí¼ ì¶”ì¶œ
            dpr_candidate = min(top_k_dpr, len(similarities))
            top_indices = np.argsort(similarities)[::-1][:dpr_candidate]
            dpr_candidates_docs = [all_documents[i] for i in top_indices]
            dpr_candidates_meta = [all_metadatas[i] for i in top_indices]

            # ì¤‘ë³µ ì œê±°ëœ DPR ê²°ê³¼
            dpr_indices_unique = [i for i in top_indices if all_documents[i] not in bm25_candidates_docs]

            # ìµœëŒ€ 3ê°œì”© BM25/DPR êµì°¨ ë°°ì¹˜
            final_results = []
            for i in range(3):
                if i < len(bm25_indices_in_current):
                    doc = bm25_candidates_docs[i]
                    meta = bm25_candidates_meta[i]
                    final_results.append((doc, meta))
        
                if i < len(dpr_indices_unique):
                    dpr_doc = all_documents[dpr_indices_unique[i]]
                    dpr_meta = all_metadatas[dpr_indices_unique[i]]
                    final_results.append((dpr_doc, dpr_meta))
            
        else:

            final_results = [
                (bm25_candidates_docs[i], bm25_candidates_meta[i]) for i in bm25_indices_in_current[:num_bm25_candidates]
            ]

        return final_results

# âœ… GPT ì‘ë‹µ ìƒì„±ê¸°
def generate_answer(query, context_docs,cat):
    
    if not context_docs: # ì°¸ê³  ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ì£¼ì„¸ìš”."

    if cat == 1:
        
        context = "\n\n".join([
            f"[{meta.get('university', 'Unknown University')} - {meta.get('major', 'Unknown Major')} - {meta.get('source_file', 'Unknown Source')}]\n{doc}"
            for doc, meta in context_docs
        ])

        prompt = (
            "ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµì˜ í•™ì‚¬ ìš”ëŒì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë”ë¼ë„ ê´€ë ¨ í•™ê³¼ ë˜ëŠ” ê·œì • ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
            "ì‚¬ìš©ìëŠ” ì•„ë˜ì˜ ì„¸ ê°€ì§€ ì „ê³µ ìœ í˜• ì¤‘ í•˜ë‚˜ì— í•´ë‹¹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ êµ¬ë¶„ì€ ëª¨ë“  í•™ê³¼ì— ë™ì¼í•˜ê²Œ ì ìš©ë˜ë©°, ì–´ë–¤ ì „ê³µì´ ì£¼ ì „ê³µì¸ì§€ì— ë”°ë¼ í•™ê³¼ë³„ ì¡¸ì—… ìš”ê±´ ë° ì´ìˆ˜ ê¸°ì¤€ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n"
            
            "1. **ë‹¨ì¼ì „ê³µ**: ì‚¬ìš©ìëŠ” íŠ¹ì • í•™ê³¼(ì˜ˆ: ì»´í“¨í„°ê³µí•™ê³¼)ë§Œ ì „ê³µí•©ë‹ˆë‹¤.\n"
            "2. **ë‹¤ì „ê³µ(ìì‹ ì˜ í•™ê³¼)**: ì‚¬ìš©ìëŠ” í•´ë‹¹ í•™ê³¼ë¥¼ ì œ1ì „ê³µìœ¼ë¡œ í•˜ê³ , ë‹¤ë¥¸ í•™ê³¼ë¥¼ ë³µìˆ˜ì „ê³µí•©ë‹ˆë‹¤.\n"
            "3. **ë‹¤ì „ê³µ(íƒ€ í•™ê³¼)**: ì‚¬ìš©ìëŠ” ë‹¤ë¥¸ í•™ê³¼ë¥¼ ì œ1ì „ê³µìœ¼ë¡œ í•˜ê³ , í•´ë‹¹ í•™ê³¼ë¥¼ ë³µìˆ˜ì „ê³µí•©ë‹ˆë‹¤.\n"

            "ì˜ˆì‹œ) ì§ˆë¬¸ì´ ì»´í“¨í„°ê³µí•™ê³¼ì— ëŒ€í•œ ê²ƒì¼ ê²½ìš°:\n"
            "- \"ë‹¨ì¼ì „ê³µ\" ì‚¬ìš©ìëŠ” ì»´í“¨í„°ê³µí•™ê³¼ë§Œ ì „ê³µ\n"
            "- \"ë‹¤ì „ê³µ(ì»´ê³µ)\" ì‚¬ìš©ìëŠ” ì»´í“¨í„°ê³µí•™ê³¼ê°€ ì œ1ì „ê³µ + ë‹¤ë¥¸ í•™ê³¼ ë³µìˆ˜ì „ê³µ\n"
            "- \"ë‹¤ì „ê³µ(íƒ€ì „ê³µ)\" ì‚¬ìš©ìëŠ” ë‹¤ë¥¸ í•™ê³¼ê°€ ì œ1ì „ê³µ + ì»´í“¨í„°ê³µí•™ê³¼ ë³µìˆ˜ì „ê³µ\n"

            "ì§ˆë¬¸ì´ ì–´ëŠ ì „ê³µ ìœ í˜•ì— í•´ë‹¹í•˜ëŠ”ì§€ ëª…í™•í•˜ì§€ ì•Šë”ë¼ë„, ê° ê²½ìš°ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ” ë‚´ìš©ì„ **ëª¨ë‘ ë¶„ë¦¬ëœ ë¬¸ë‹¨**ìœ¼ë¡œ ë‚˜ëˆ  ì„¤ëª…í•˜ì„¸ìš”.\n"
            "ì œê³µëœ contextì—ì„œ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ ì°¾ì„ ìˆ˜ ì—†ë‹¤ê³  ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•´ì£¼ì„¸ìš”.\n"
            
        )

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"context:\n{context}\n\nì§ˆë¬¸: {query}\në‹µë³€:"}
        ]
        model_name = "gpt-4o"
    
    elif cat == 2:
        context = "\n\n".join([
            f"[{meta.get('university', 'Unknown University')} - {meta.get('major', 'Unknown Major')} - {meta.get('source_file', 'Unknown Source')}]\n{doc}"
            for doc, meta in context_docs
        ])

        '''for i, (doc, meta) in enumerate(context_docs):
            preview = doc[:100].replace("\n", " ")  # ì¤„ë°”ê¿ˆ ì œê±°ë¡œ ë³´ê¸° ì¢‹ê²Œ
            print(f"[{i}] {meta.get('university', 'Unknown University')} - {meta.get('major', 'Unknown Major')} - {meta.get('source_file', 'Unknown Source')}]")
            print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {preview}\n")'''

        

        prompt = (
            "ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµì˜ í•™ì‚¬ ìš”ëŒ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "- ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë”ë¼ë„, ê´€ë ¨ í•™ê³¼ ë˜ëŠ” ê·œì • ë¬¸ì„œë¥¼ ëª¨ë‘ ì°¸ê³ í•˜ì—¬ ê°€ëŠ¥í•œ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”.\n"
            "- ê°™ì€ ê³¼ëª©ì— ëŒ€í•œ ì„¤ëª…ì´ ì—¬ëŸ¬ í•™ê³¼ ë˜ëŠ” ì „ê³µì—ì„œ ë°˜ë³µë  ê²½ìš°, **ëª¨ë“  ê´€ë ¨ ë¬¸ì„œì—ì„œ ë‚˜ì˜¨ ì„¤ëª…ì„ ë¹ ì§ì—†ì´ í¬í•¨**í•˜ì„¸ìš”.\n"
            "- ê°ê°ì˜ ì„¤ëª…ì€ **ì¶œì²˜ í•™ê³¼ëª… ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ì„ ë¶„ë¦¬í•˜ì—¬ ì¶œë ¥**í•˜ê³ , ì¤‘ë³µëœ ë‚´ìš©ì´ ìˆë”ë¼ë„ **í•™ê³¼ ë¬¸ë§¥ ë‚´ì—ì„œëŠ” ìƒëµí•˜ì§€ ë§ê³  ëª¨ë‘ ì¶œë ¥**í•˜ì„¸ìš”.\n"
            "- ìš”ì•½í•˜ì§€ ë§ˆì„¸ìš”. **ëª¨ë“  í•™ê³¼ë³„ ì„¤ëª…ì„ ì „ë¶€ ë‚˜ì—´**í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n"
            "- ì œê³µëœ contextì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ê²½ìš°, \"ì œê³µëœ ì •ë³´ì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"ë¼ê³  ì¶œë ¥í•˜ì„¸ìš”.\n"

            
        )

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"context:\n{context}\n\nì§ˆë¬¸: {query}\në‹µë³€:"}
        ]
        #model_name = "ft:gpt-3.5-turbo-0125:capston::Bdtr05OS"
        model_name="gpt-4o"

    else:
        
        context = context_docs
        prompt = (
            "ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµì˜ ê³µì§€ì‚¬í•­ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
            #"ë‹¤ìŒ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ë˜, ë§í¬ëŠ” í•œ ë²ˆë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n"
            "ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë”ë¼ë„, ì œê³µëœ ê³µì§€ contextë¥¼ ë°”íƒ•ìœ¼ë¡œ ê·œì •ê³¼ ì‚¬ì‹¤ì— ê·¼ê±°í•´ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "ê°€ëŠ¥í•œ í•œ ì§ˆë¬¸ê³¼ í‚¤ì›Œë“œê°€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê³µì§€ë¥¼ ì°¾ì•„ì„œ ì œì‹œí•˜ì„¸ìš”.\n"
            "ì—¬ëŸ¬ ê°œì˜ ê³µì§€ê°€ ê´€ë ¨ ìˆë‹¤ë©´, ë‚ ì§œ(date)ê°€ ê°€ì¥ ìµœì‹ ì¸ ê³µì§€ê°€ ì•ì— ì˜¤ë„ë¡ ì •ë ¬í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”.\n"
            "ì œê³µëœ contextì— ê´€ë ¨ ì •ë³´ê°€ ì—†ë‹¤ë©´, 'ê´€ë ¨ ê³µì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n"
            "ë§í¬ëŠ” ë°˜ë“œì‹œ í•œ ë²ˆë§Œ ì¶œë ¥í•˜ê³ , ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì§€ ë§ê³  ìˆœìˆ˜í•œ URLë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n\n"
        )


        messages = [
            {"role": "system", "content": prompt},
            # ğŸŸ¡ One-shot ì˜ˆì‹œ
            {"role": "user", "content": "context:\n[ì¡¸ì—…] 2023í•™ë…„ë„ í›„ê¸°(2024ë…„ 8ì›”) ì¡¸ì—…_í•™ìœ„ì¦ ë°°ë¶€ ë° í•™ìœ„ê°€ìš´ ëŒ€ì—¬ ì•ˆë‚´|2024.07.30|https://sogang.ac.kr/ko/detail/\n\nì§ˆë¬¸: í•™ìœ„ ê°€ìš´ì€ ì–´ë””ì„œ ëŒ€ì—¬í•  ìˆ˜ ìˆì–´?\në‹µë³€:"},
            {"role": "assistant", "content": "í•™ìœ„ ê°€ìš´ ëŒ€ì—¬ì™€ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒ ê³µì§€ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\nì œëª©:[ì¡¸ì—…] 2023í•™ë…„ë„ í›„ê¸°(2024ë…„ 8ì›”) ì¡¸ì—…_í•™ìœ„ì¦ ë°°ë¶€ ë° í•™ìœ„ê°€ìš´ ëŒ€ì—¬ ì•ˆë‚´\nì—…ë¡œë“œì¼ì: 2024.07.30\në§í¬:https://sogang.ac.kr/ko/detail/\n"},
            {"role": "user", "content": f"context:\n{context}\n\nì§ˆë¬¸: {query}\në‹µë³€:"}
        ]
        model_name = "gpt-4o"

    total_tokens = count_total_tokens(messages, model="gpt-4o")
    print(total_tokens)
    max_tokens_model = 128000 # ëª¨ë¸ì˜ ìµœëŒ€ í† í° (gpt-4o ê¸°ì¤€)
    max_response_tokens = 4096 # ë‹µë³€ìœ¼ë¡œ ë°›ê³ ì í•˜ëŠ” ìµœëŒ€ í† í° ìˆ˜

    # í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ ê²½ìš°, ëª¨ë¸ì˜ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ìë¥´ê±°ë‚˜,
    # ë‹µë³€ ìƒì„± í† í° ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ ì…ë ¥ í† í°ì„ ì¡°ì ˆí•´ì•¼ í•¨.
    # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ ê²½ê³ ë§Œ ì¶œë ¥
    if total_tokens > max_tokens_model - max_response_tokens : # ëª¨ë¸ í•œê³„ - ì‘ë‹µ í† í° = í”„ë¡¬í”„íŠ¸ ìµœëŒ€
        print(f"âš ï¸ ì „ì²´ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜ê°€ {total_tokens}ê°œë¡œ ëª¨ë¸ ì œí•œ({max_tokens_model})ì„ ì´ˆê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë¶€ ì»¨í…ìŠ¤íŠ¸ê°€ ì˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        # ì‹¤ì œë¡œëŠ” contextë¥¼ ì¤„ì´ëŠ” ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.
    try:
        response = openai.ChatCompletion.create(
            model=model_name,
            messages=messages,
            max_tokens=4096, # ë‹µë³€ í† í° ìˆ˜ ì œí•œ
            temperature = 0.3,
            top_p = 0.9
        )
        return response.choices[0].message['content'] # ìˆ˜ì •: .message.content -> .message['content']
    except openai.error.InvalidRequestError as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # í† í° ì´ˆê³¼ ì—ëŸ¬ì˜ ê²½ìš°, ì—¬ê¸°ì„œ contextë¥¼ ì¤„ì—¬ì„œ ì¬ì‹œë„í•˜ëŠ” ë¡œì§ì„ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        if "maximum context length" in str(e):
            return "ì§ˆë¬¸ê³¼ ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” ì§§ê²Œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜, í•„í„°ë§ì„ í†µí•´ ë¬¸ì„œ ë²”ìœ„ë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”."
        return "ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    except Exception as e: # ë‹¤ë¥¸ ì˜ˆì™¸ ì²˜ë¦¬
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


#query preprocess module
KOREAN_PARTICLE_PATTERN = r'(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ìœ¼ë¡œ|ë¡œ|ë„|ë§Œ|ê¹Œì§€|ë¶€í„°|ì¡°ì°¨|ì¸ë°|ê³ |ì™€|ê³¼|ë§ˆì €|ì²˜ëŸ¼|ê»˜ì„œ|ë°–ì—|ì´ë©°|ì´ê³ |ì´ë‚˜|ë¼ë„|ë¼ê³ |ë¼ëŠ”|ë“ ì§€|ë§Œí¼|ì•¼)?'

def preprocess_query(query):
    used_majors = []
    replaced_ranges = []

    for full_name, variants in ABBREVIATION_GROUPS.items():
        for variant in sorted(variants, key=lambda x: -len(x)):
            # ì¡°ì‚¬ ë˜ëŠ” ë„ì–´ì“°ê¸°ë§Œ í—ˆìš© (ì˜ˆ: 'ë¬¼ë¦¬ ', 'ë¬¼ë¦¬ëŠ”')
            pattern = re.compile(rf'(?<!\w)({re.escape(variant)})(?=(\s|ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì˜|ì™€|ê³¼|ë„|ì—ì„œ|ì—ê²Œ|ë¶€í„°|ê¹Œì§€|ë³´ë‹¤|ì²˜ëŸ¼|ì¡°ì°¨|ë§ˆì €|ë§Œ|ì”©|ë“ ì§€|ì•¼|ì•¼ë§ë¡œ|ì´ë‚˜|ë‚˜|ì´ë©°|ë¡œì„œ|ìœ¼ë¡œ|ë¡œ|ì—|ì—ì„œ|í•œí…Œ|ë°–ì—|ë¿|ì´ë¼ë„))')

            def replacer(m):
                start, end = m.start(1), m.end(1)
                for r_start, r_end in replaced_ranges:
                    if not (end <= r_start or start >= r_end):
                        return m.group(0)
                replaced_ranges.append((start, start + len(full_name)))
                if full_name not in used_majors:
                    used_majors.append(full_name)
                return full_name

            query = pattern.sub(replacer, query)

    return query


    
def extract_date_key_from_query(query: str) -> str | None:
    """
    queryì— DATE_GROUPSì˜ value ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ í•´ë‹¹ keyë¥¼ ë°˜í™˜.
    ì—†ìœ¼ë©´ None ë°˜í™˜.
    """
    for key, phrases in DATE_GROUPS.items():
        for phrase in phrases:
            if phrase in query:
                return key
    return None

def extract_relate_query(query, context_docs):
    prompt = (
    "ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµì˜ í•™ì‚¬ ìš”ëŒ ë° ê³µì§€ì‚¬í•­ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë™í•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤.\n"
    "- ì•„ë˜ì— ì£¼ì–´ì§„ contextëŠ” ê³¼ê±° ê³µì§€ì‚¬í•­ ë° í•™ì‚¬ ìš”ëŒì˜ ì¼ë¶€ì…ë‹ˆë‹¤.\n"
    "- ì‚¬ìš©ìë¡œë¶€í„°ì˜ queryëŠ” ê·¸ ë‹¤ìŒì— ì£¼ì–´ì§‘ë‹ˆë‹¤.\n"
    "- ì´ queryì™€ ê´€ë ¨í•˜ì—¬ ì‚¬ìš©ìê°€ í›„ì†ìœ¼ë¡œ ê¶ê¸ˆí•´í•  ìˆ˜ ìˆëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸ 3ê°€ì§€ë¥¼ í•œêµ­ì–´ë¡œ ì œì•ˆí•˜ì„¸ìš”.\n"
    "- ì£¼ì–´ì§„ contextì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ì§ˆë¬¸ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n"
    "- ì§ˆë¬¸ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ë©°, ë¦¬ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.\n\n"
    )

    messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"query: {query}\n context:{context_docs}\n ì¶”ì²œ ì§ˆë¬¸:"}
        ]

    try:
        response = openai.ChatCompletion.create(
            model="gpt-4o",
            messages=messages,
            max_tokens=4096, # ë‹µë³€ í† í° ìˆ˜ ì œí•œ
            temperature = 0.3,
            top_p = 0.9
        )
        return response.choices[0].message['content']
    except Exception as e:
        print(f"âŒ ì¶”ì²œ ì§ˆë¬¸ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        return "ì¶”ì²œ ì§ˆë¬¸ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


# âœ… ë©”ì¸ ì‹¤í–‰ ë£¨í”„
if __name__ == "__main__":
    print("ğŸ’¬ í•™ì‚¬ìš”ëŒ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì‹œì‘ë¨.")

    collection_data = load_corpus_by_collection()
    if not collection_data:
        print("âš ï¸ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. DBë¥¼ ë¨¼ì € ìƒì„±í•˜ê±°ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()

    # âœ… ì»¬ë ‰ì…˜ë³„ retriever ì´ˆê¸°í™”
    retrievers = {}
    for col_name, content in collection_data.items():
        retrievers[col_name] = HybridRetriever(
            content["documents"],
            content["metadatas"],
            collection_name=col_name
        )

    # âœ… major ëª©ë¡ë„ í•¨ê»˜ ì €ì¥
    majors_by_collection = {
        col_name: content["majors"]
        for col_name, content in collection_data.items()
        if "majors" in content
    }   

    # âœ… ì¹´í…Œê³ ë¦¬ â†’ ì»¬ë ‰ì…˜ ì´ë¦„ ë§¤í•‘
    category_to_collection = {
        "1": "collection_course_json",
        "2": "collection_sub",
        "3": "collection_notice_md"
    }

    top_docs_with_meta = None  # âœ… ì´ˆê¸°í™”
    suggested_question_list = []  # âœ… ì¶”ì²œ ì§ˆë¬¸ ì €ì¥ìš©
    is_rec_mode = False  # âœ… ì¶”ì²œ ì§ˆë¬¸ ì„ íƒ ëª¨ë“œ ì—¬ë¶€

    while True:
        #category ì´ˆê¸°í™”
        print("\nì–´ë–¤ ì¹´í…Œê³ ë¦¬ì˜ ì§ˆë¬¸ì„ í• ì§€ ê³¨ë¼ì£¼ì„¸ìš”.")
        cat = input("\n1. ê³¼ëª©/ì „ê³µ ì´ìˆ˜ ìš”ê±´ 2. ê³¼ëª© ì •ë³´ 3. í•™ì‚¬ ê³µì§€\n-> ")

        if cat not in category_to_collection:
            print("âš ï¸ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. 1, 2 ë˜ëŠ” 3ì„ ì…ë ¥í•˜ì„¸ìš”.")
            continue
        else:
            break
    
    selected_collection = category_to_collection[cat]

    if selected_collection not in retrievers:
        print(f"âŒ ì„ íƒí•œ ì»¬ë ‰ì…˜ '{selected_collection}'ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        exit()

    retriever = retrievers[selected_collection]

    if selected_collection != "collection_notice_md":
        unique_majors = majors_by_collection[selected_collection]

    date_keyword = "2025-1"


    while True:
        if is_rec_mode:
            query = input("\nğŸ” ì¶”ì²œ ì§ˆë¬¸ ë²ˆí˜¸(1~3)ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì·¨ì†Œí•˜ë ¤ë©´ cancelì„ ì…ë ¥í•´ì£¼ì„¸ìš”.): ").strip()
    
            if query.lower() == "cancel":
                print("ì¶”ì²œ ì§ˆë¬¸ ì„ íƒì„ ì·¨ì†Œí–ˆìŠµë‹ˆë‹¤.")
                is_rec_mode = False
                continue

            if query in {"1", "2", "3"} and suggested_question_list:
                selected_idx = int(query) - 1
                if selected_idx < len(suggested_question_list):
                    query = suggested_question_list[selected_idx]
                    print(f"\nâ¡ï¸ ì„ íƒëœ ì¶”ì²œ ì§ˆë¬¸ìœ¼ë¡œ ê³„ì†í•©ë‹ˆë‹¤: \"{query}\"\n")
                    is_rec_mode = False
                else:
                    print("âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ ë²ˆí˜¸ì…ë‹ˆë‹¤.")
                    continue
            else:
                print("âš ï¸ ì¶”ì²œ ì§ˆë¬¸ ë²ˆí˜¸(1~3) ë˜ëŠ” cancel ì¤‘ í•˜ë‚˜ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
        else:
            query = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œë¥¼ ì›í•˜ë©´ exitì„, category ë³€ê²½ì„ ì›í•˜ë©´ catì„ ì…ë ¥í•´ì£¼ì„¸ìš”.): ")

            if query.lower().strip() == "exit":
                print("ğŸš«ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")
                break

            elif query.lower().strip() == "cat":
                top_docs_with_meta = None  # âœ… ì´ˆê¸°í™”
                suggested_question_list = []
                is_rec_mode = False
                print("\n contextë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
                print("\nì–´ë–¤ ì¹´í…Œê³ ë¦¬ì˜ ì§ˆë¬¸ì„ í• ì§€ ê³¨ë¼ì£¼ì„¸ìš”.")
                cat = input("\n1. ê³¼ëª©/ì „ê³µ ì´ìˆ˜ ìš”ê±´ 2. ê³¼ëª© ì •ë³´ 3. í•™ì‚¬ ê³µì§€\n-> ")

                if cat not in category_to_collection:
                    print("âš ï¸ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. 1, 2 ë˜ëŠ” 3ì„ ì…ë ¥í•˜ì„¸ìš”.")
                    continue
    
                selected_collection = category_to_collection[cat]

                if selected_collection not in retrievers:
                    print(f"âŒ ì„ íƒí•œ ì»¬ë ‰ì…˜ '{selected_collection}'ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    break

                retriever = retrievers[selected_collection]
                if selected_collection != "collection_notice_md":
                    unique_majors = majors_by_collection[selected_collection]
                continue 
                #query = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œë¥¼ ì›í•˜ë©´ exitì„, category ë³€ê²½ì„ ì›í•˜ë©´ catì„ ì…ë ¥í•´ì£¼ì„¸ìš”.): ")

            elif query.lower() == "rec":
                if not suggested_question_list:
                    print("âŒ ì•„ì§ ì¶”ì²œ ì§ˆë¬¸ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì¼ë°˜ ì§ˆë¬¸ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
                    continue
                else:
                    is_rec_mode = True
                    continue

        if selected_collection == "collection_notice_md":
            #ì‹œê¸° ì¶”ì¶œ
            date_key = extract_date_key_from_query(query)
            

             # ê¸°ë³¸ê°’ ì„¤ì •
            if date_key is None:
                print(f"â„¹ï¸ íŠ¹ì • ì‹œê¸° í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. '{date_keyword}' ë¬¸ì„œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            else:
                date_keyword = date_key
                print(f"âœ¨ '{date_keyword}' ê´€ë ¨ ì •ë³´ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

            query = query.strip()+" "+date_keyword+"ì™€ ê´€ë ¨ëœ ê³µì§€ë¥¼ ì°¾ì•„ì¤˜."
            #print(query)

            # âœ… í•´ë‹¹ ì‹œê¸°ì˜ ë¬¸ì„œë§Œ í•„í„°ë§
            all_docs = collection_data[selected_collection]["documents"]
            all_metas = collection_data[selected_collection]["metadatas"]

            answer=generate_answer(query, all_docs, cat=3)
            context_text = "\n\n".join(all_docs)
            suggested_questions_text = extract_relate_query(query, context_text)

        elif selected_collection == "collection_sub":
            # 1) ì¶•ì•½ì–´ ê·¸ë£¹ ì¹˜í™˜ ì ìš©
            query = preprocess_query(query)

            # 2) ë³€í™˜ëœ ì§ˆì˜ë¡œ í•™ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
            major_filter_keyword = extract_major_keyword(query, unique_majors,threshold = 80)

            if major_filter_keyword:
                print(f"âœ¨ '{major_filter_keyword}' ê´€ë ¨ ì •ë³´ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
                # 3) í•„í„°ë§ í‚¤ì›Œë“œë¥¼ retrieverì— ì „ë‹¬
                top_docs_with_meta = retriever.retrieve(query, top_k_bm25=3, top_k_dpr=3, filter_major=major_filter_keyword,alpha=0.5,cat= 2)
            else:
                if not top_docs_with_meta:
                    print("â„¹ï¸ íŠ¹ì • í•™ê³¼ í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
                    # 3) í•„í„°ë§ í‚¤ì›Œë“œë¥¼ retrieverì— ì „ë‹¬
                    top_docs_with_meta = retriever.retrieve(query, top_k_bm25=3, top_k_dpr=3, filter_major=major_filter_keyword,alpha=0.5,cat= 2)
                elif extract_subject_by_rapidfuzz(query):
                    print("â„¹ï¸ íŠ¹ì • ê³¼ëª© í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. contextë¥¼ ìƒˆë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
                    top_docs_with_meta = retriever.retrieve(query, top_k_bm25=3, top_k_dpr=3, filter_major=major_filter_keyword,alpha=0.5,cat= 2)

            print(f"query: {query}")
        

            if not top_docs_with_meta:
                print("\nğŸ§  chatbot ì‘ë‹µ:\nê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                continue # ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ

            answer = generate_answer(query, top_docs_with_meta, cat=2)
            context_text = "\n\n".join([doc for doc, _ in top_docs_with_meta])
            suggested_questions_text = extract_relate_query(query, context_text)

            print("\nğŸ“ ì°¸ê³ í•œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:")
            for doc_content, meta in top_docs_with_meta: # ë¬¸ì„œ ë‚´ìš©ë„ í•¨ê»˜ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                # print(f" - (ë‚´ìš© ì¼ë¶€: {doc_content[:50]}...) ë©”íƒ€ë°ì´í„°: {meta}")
                print(f" - ë©”íƒ€ë°ì´í„°: {meta}")
        else:
            # 1) ì¶•ì•½ì–´ ê·¸ë£¹ ì¹˜í™˜ ì ìš©
            query = preprocess_query(query)

            print(f"query: {query}")

            # 2) ë³€í™˜ëœ ì§ˆì˜ë¡œ í•™ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
            major_filter_keyword = extract_major_keyword(query, unique_majors,threshold = 60)

            if major_filter_keyword:
                print(f"âœ¨ '{major_filter_keyword}' ê´€ë ¨ ì •ë³´ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
                # 3) í•„í„°ë§ í‚¤ì›Œë“œë¥¼ retrieverì— ì „ë‹¬
                top_docs_with_meta = retriever.retrieve(query, top_k_bm25=10, top_k_dpr=3, filter_major=major_filter_keyword,cat=1)
            else:
                print("â„¹ï¸ íŠ¹ì • í•™ê³¼ í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        

            if not top_docs_with_meta:
                print("\nğŸ§  chatbot ì‘ë‹µ:\nê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                continue # ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ

            answer = generate_answer(query, top_docs_with_meta, cat=1)
            context_text = "\n\n".join([doc for doc, _ in top_docs_with_meta])
            suggested_questions_text = extract_relate_query(query, context_text)

            print("\nğŸ“ ì°¸ê³ í•œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:")
            for doc_content, meta in top_docs_with_meta: 
                print(f" - ë©”íƒ€ë°ì´í„°: {meta}")

        print(f"\nğŸ§  chatbot ì‘ë‹µ:\n{answer}")
        # ë¦¬ìŠ¤íŠ¸ë¡œ ë¶„ë¦¬ (ìˆ«ì bullet ì œê±° + strip)
        suggested_question_list = [
            line.lstrip("1234567890.â—â€¢- ").strip()
            for line in suggested_questions_text.strip().splitlines()
            if line.strip()
        ]
        print("\nğŸ“Œ ê´€ë ¨ ì¶”ì²œ ì§ˆë¬¸:")
        for idx, q in enumerate(suggested_question_list, 1):
            print(f"{idx}. {q}")
        print("â¡ï¸ ì¶”ì²œ ì§ˆë¬¸ ì„ íƒì„ ì›í•˜ë©´ 'rec'ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")