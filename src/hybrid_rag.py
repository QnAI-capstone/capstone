from config import OPENAI_API_KEY
import openai
import tiktoken
import numpy as np
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from chromadb import PersistentClient
from chromadb.utils.embedding_functions import EmbeddingFunction
import re # ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©ì„ ìœ„í•´ ì¶”ê°€
from rapidfuzz import process
from dictionary import ABBREVIATION_GROUPS,DATE_GROUPS
import math
from ex_sub import extract_subject_by_rapidfuzz

# âœ… API í‚¤
openai.api_key = OPENAI_API_KEY

# âœ… í•œêµ­ì–´ ì„ë² ë”© í•¨ìˆ˜
class KoSimCSEEmbedding(EmbeddingFunction):
    def __init__(self):
        self.model = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
    def __call__(self, input):
        return self.model.encode(input).tolist()

#notice -> ìˆ˜ì •
def load_corpus_by_collection():
    """
    ChromaDBì˜ ëª¨ë“  ì»¬ë ‰ì…˜ì„ ìˆœíšŒí•˜ì—¬,
    ê° ì»¬ë ‰ì…˜ ì´ë¦„ì„ keyë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•´ ë°˜í™˜í•©ë‹ˆë‹¤.

    Returns:
        dict[str, dict]: {
            collection_name: {
                "documents": [...],
                "metadatas": [...],
                "majors": [...]
            },
            ...
        }
    """
    client = PersistentClient(path="./chroma_store")
    embedding_fn = KoSimCSEEmbedding()
    collections_info = client.list_collections()

    print(f"ì´ {len(collections_info)}ê°œì˜ ì»¬ë ‰ì…˜ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.")

    result = {}

    for col_info in collections_info:
        collection_name = col_info.name
        collection = client.get_collection(name=collection_name, embedding_function=embedding_fn)
        data = collection.get(include=["documents", "metadatas"])

        documents = data["documents"]
        metadatas = data["metadatas"]

        #ê³µì§€ ë°ì´í„° ì²˜ë¦¬
        if collection_name == "collection_notice":
            dates = list({meta.get("date") for meta in metadatas if meta and "date" in meta})
            result[collection_name] = {
                "documents": documents,
                "metadatas": metadatas,
                "dates": dates
            }
            print(f"ğŸ“Œ 'notice_collection' ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ. ë¬¸ì„œ ìˆ˜: {len(documents)}")

        
        #ê³¼ëª© ì´ìˆ˜, ê³¼ëª© ì„¤ëª… ë°ì´í„° ì²˜ë¦¬
        else:
            majors = list({meta["major"] for meta in metadatas if meta and "major" in meta})

            result[collection_name] = {
                "documents": documents,
                "metadatas": metadatas,
                "majors": majors
            }

            print(f"âœ… '{collection_name}' ì»¬ë ‰ì…˜ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ. ë¬¸ì„œ ìˆ˜: {len(documents)}")

    return result

def count_total_tokens(messages, model="gpt-4o"):
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")

    total_tokens = 0
    for message in messages:
        total_tokens += 4
        for key, value in message.items():
            total_tokens += len(encoding.encode(value))
    total_tokens += 2
    return total_tokens

# âœ… ì‚¬ìš©ì ì§ˆì˜ì—ì„œ í•™ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜
def extract_major_keyword(query, majors_list,threshold=70):
    """
    ì‚¬ìš©ì ì§ˆì˜ì—ì„œ ì–¸ê¸‰ëœ í•™ê³¼ í‚¤ì›Œë“œë¥¼ majors_list (DBì— ì €ì¥ëœ ì‹¤ì œ í•™ê³¼ëª…) ê¸°ì¤€ìœ¼ë¡œ ìœ ì‚¬ ë¬¸ìì—´ ë§¤ì¹­í•˜ì—¬ ì¶”ì¶œí•©ë‹ˆë‹¤.
    ë„ì–´ì“°ê¸°, ì˜¤íƒ€, ì¶•ì•½ì–´ ì°¨ì´ ë“±ìœ¼ë¡œ ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šì•„ë„ ê°€ì¥ ìœ ì‚¬í•œ í•™ê³¼ëª…ì„ ì°¾ì•„ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    # ì‚¬ìš©ì ì§ˆì˜ ì •ê·œí™”: ê³µë°± ì œê±°, ì†Œë¬¸ì ë³€í™˜
    normalized_query = query.replace(" ", "").lower()

    # majors_listì˜ í•™ê³¼ëª…ë„ ì •ê·œí™” (ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°, ì†Œë¬¸ì ë³€í™˜)
    candidates = [m.replace("_", "").lower() for m in majors_list]

    # rapidfuzzì˜ extractOneìœ¼ë¡œ ê°€ì¥ ìœ ì‚¬í•œ í•™ê³¼ëª…ê³¼ ìœ ì‚¬ë„ ë°˜í™˜
    best_match = process.extractOne(normalized_query, candidates)
    
    # âœ… ìœ ì‚¬í•œ í•­ëª© ì—¬ëŸ¬ ê°œ ì¶”ì¶œ
    matches = process.extract(normalized_query, candidates, limit=2)
    # âœ… ìœ ì‚¬ë„ ê¸°ì¤€ í†µê³¼í•œ í•™ê³¼ë§Œ ë°˜í™˜
    result = []
    for match_str, score, _ in matches:
        idx = candidates.index(match_str)
        print(majors_list[idx])
        if score >= threshold:
            idx = candidates.index(match_str)
            matched_major = majors_list[idx]
            result.append(matched_major)
            print(f"âœ… ìœ ì‚¬ë„ {score} â†’ ë§¤ì¹­ëœ í•™ê³¼: {matched_major}")
        else:
            print(f"âŒ ìœ ì‚¬ë„ {score} â†’ ë¬´ì‹œë¨")
        
    return result  # ìµœëŒ€ top_kê°œì˜ í•™ê³¼ëª… ë°˜í™˜



# âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ê¸° ì´ˆê¸°í™”
class HybridRetriever:
    def __init__(self, corpus_all, metadatas_all,collection_name):
        # ì´ˆê¸°í™” ì‹œì ì—ëŠ” ì „ì²´ ë°ì´í„°ë¥¼ ë³´ê´€
        self.corpus_all = corpus_all
        self.metadatas_all = metadatas_all
        self.encoder = SentenceTransformer("snunlp/KR-SBERT-V40K-klueNLI-augSTS")
        self.collection_name = collection_name

    def retrieve(self, query, top_k_bm25=10, top_k_dpr=3, filter_major=None,alpha=0.5,cat=1):
        # ì‹¤ì œ ê²€ìƒ‰ ëŒ€ìƒì´ ë  ì½”í¼ìŠ¤ì™€ ë©”íƒ€ë°ì´í„°
        current_corpus = self.corpus_all
        current_metadatas = self.metadatas_all

        # í•„í„°ë§í•  í•™ê³¼ê°€ ì§€ì •ëœ ê²½ìš°
        if filter_major:
            print(f"ğŸ” '{filter_major}' í•™ê³¼ ê´€ë ¨ ë¬¸ì„œë¡œ í•„í„°ë§ ì¤‘...")
            filtered_indices = [
                i for i, meta in enumerate(self.metadatas_all)
                if meta and meta.get('major') in filter_major #filter_majorë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°”ê¿ˆ
            ]
            if not filtered_indices:
                print(f"âš ï¸ '{filter_major}' í•™ê³¼ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            else:
                current_corpus = [self.corpus_all[i] for i in filtered_indices]
                current_metadatas = [self.metadatas_all[i] for i in filtered_indices]
                print(f"ğŸ” í•„í„°ë§ ê²°ê³¼: ì´ {len(current_corpus)}ê°œì˜ ë¬¸ì„œë¡œ ì œí•œë¨.")


        if not current_corpus: # í•„í„°ë§ í›„ ë¬¸ì„œê°€ ì—†ì„ ê²½ìš°
             print("âš ï¸ ê²€ìƒ‰í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
             return []

        # BM25 ê³„ì‚° (í•„í„°ë§ëœ ì½”í¼ìŠ¤ ë˜ëŠ” ì „ì²´ ì½”í¼ìŠ¤ ëŒ€ìƒ)
        tokenized_corpus = [doc.split() for doc in current_corpus]
        
        bm25 = BM25Okapi(tokenized_corpus,b=0.25)
        tokenized_query = query.split()
        bm25_scores = bm25.get_scores(tokenized_query)

        # BM25 ê²°ê³¼ê°€ top_k_bm25ë³´ë‹¤ ì ì„ ê²½ìš°ë¥¼ ëŒ€ë¹„
        num_bm25_candidates = min(top_k_bm25, len(current_corpus))
        bm25_indices_in_current = np.argsort(bm25_scores)[::-1][:num_bm25_candidates]
        bm25_candidates_docs = [current_corpus[i] for i in bm25_indices_in_current]
        bm25_candidates_meta = [current_metadatas[i] for i in bm25_indices_in_current]

        # -------------------------------
        # 2ï¸âƒ£ ChromaDB ë²¡í„° ë¶ˆëŸ¬ì™€ ì˜ë¯¸ ìœ ì‚¬ë„ ê³„ì‚°
        # -------------------------------
        
        embedding_fn = KoSimCSEEmbedding()
        client = PersistentClient(path="./chroma_store")
        collection = client.get_collection(name=self.collection_name, embedding_function=embedding_fn)

        # BM25 í›„ë³´ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œ ID ìƒì„±
        bm25_candidate_ids = []
        if cat == 1:
            for meta in bm25_candidates_meta:
                file = meta.get("source_file", "")
                university = meta.get("university", "")
                major = meta.get("major", "")
                bm25_candidate_ids.append(f"{file}_{university}_{major}")
        else:
            bm25_candidate_ids = [meta["id"] for meta in bm25_candidates_meta if "id" in meta]

        # í•´ë‹¹ IDë“¤ì˜ ì„ë² ë”© ë¶ˆëŸ¬ì˜¤ê¸°
        retrieved = collection.get(ids=bm25_candidate_ids, include=["embeddings"])
        candidate_embeddings = retrieved["embeddings"]

        # ì§ˆì˜ ì„ë² ë”© ìƒì„±
        query_embedding = self.encoder.encode([query])
        similarities = cosine_similarity(query_embedding, candidate_embeddings)[0]

        # Top-K DPR ìœ ì‚¬ë„ ì •ë ¬
        num_dpr_candidates = min(top_k_dpr, len(bm25_candidates_docs))
        top_indices_in_bm25 = np.argsort(similarities)[::-1][:num_dpr_candidates]

        # âœ… í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜ ê³„ì‚°
        bm25_top_scores = np.array([bm25_scores[idx] for idx in bm25_indices_in_current])
        bm25_norm = (bm25_top_scores - np.min(bm25_top_scores)) / (np.max(bm25_top_scores) - np.min(bm25_top_scores) + 1e-8)
        dpr_norm = (similarities - np.min(similarities)) / (np.max(similarities) - np.min(similarities) + 1e-8)

        hybrid_scores = alpha * bm25_norm + (1 - alpha) * dpr_norm
        top_indices = np.argsort(hybrid_scores)[::-1][:top_k_dpr]

        sorted_indices = np.argsort(hybrid_scores)[::-1]  # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ ì¸ë±ìŠ¤

        for rank, i in enumerate(sorted_indices):
            raw_bm25_score = bm25_top_scores[i]
            raw_dpr_score = similarities[i]
    
            print(f"  [{rank}] í†µí•©ì ìˆ˜: {hybrid_scores[i]:.4f} | "
                f"BM25(norm): {bm25_norm[i]:.4f} | BM25(raw): {raw_bm25_score:.4f} | "
                f"DPR(norm): {dpr_norm[i]:.4f} | DPR(raw): {raw_dpr_score:.4f} | "
                f"ë©”íƒ€: {bm25_candidates_meta[i]}")


        final_results = [
            (bm25_candidates_docs[i], bm25_candidates_meta[i]) for i in sorted_indices[:top_k_dpr]
        ]


        return final_results

# âœ… GPT ì‘ë‹µ ìƒì„±ê¸°
def generate_answer(query, context_docs,cat):
    
    if not context_docs: # ì°¸ê³  ë¬¸ì„œê°€ ì—†ëŠ” ê²½ìš°
        return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ í•´ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ì£¼ì„¸ìš”."

    if cat == 1:
        
        context = "\n\n".join([
            f"[{meta.get('university', 'Unknown University')} - {meta.get('major', 'Unknown Major')} - {meta.get('source_file', 'Unknown Source')}]\n{doc}"
            for doc, meta in context_docs
        ])

        prompt = (
            "ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµì˜ í•™ì‚¬ ìš”ëŒì— ê¸°ë°˜í•˜ì—¬ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë”ë¼ë„ ê´€ë ¨ í•™ê³¼ ë˜ëŠ” ê·œì • ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì •í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.\n"
            "ì‚¬ìš©ìëŠ” ì•„ë˜ì˜ ì„¸ ê°€ì§€ ì „ê³µ ìœ í˜• ì¤‘ í•˜ë‚˜ì— í•´ë‹¹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ êµ¬ë¶„ì€ ëª¨ë“  í•™ê³¼ì— ë™ì¼í•˜ê²Œ ì ìš©ë˜ë©°, ì–´ë–¤ ì „ê³µì´ ì£¼ ì „ê³µì¸ì§€ì— ë”°ë¼ í•™ê³¼ë³„ ì¡¸ì—… ìš”ê±´ ë° ì´ìˆ˜ ê¸°ì¤€ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n"
            
            "1. **ë‹¨ì¼ì „ê³µ**: ì‚¬ìš©ìëŠ” íŠ¹ì • í•™ê³¼(ì˜ˆ: ì»´í“¨í„°ê³µí•™ê³¼)ë§Œ ì „ê³µí•©ë‹ˆë‹¤.\n"
            "2. **ë‹¤ì „ê³µ(ìì‹ ì˜ í•™ê³¼)**: ì‚¬ìš©ìëŠ” í•´ë‹¹ í•™ê³¼ë¥¼ ì œ1ì „ê³µìœ¼ë¡œ í•˜ê³ , ë‹¤ë¥¸ í•™ê³¼ë¥¼ ë³µìˆ˜ì „ê³µí•©ë‹ˆë‹¤.\n"
            "3. **ë‹¤ì „ê³µ(íƒ€ í•™ê³¼)**: ì‚¬ìš©ìëŠ” ë‹¤ë¥¸ í•™ê³¼ë¥¼ ì œ1ì „ê³µìœ¼ë¡œ í•˜ê³ , í•´ë‹¹ í•™ê³¼ë¥¼ ë³µìˆ˜ì „ê³µí•©ë‹ˆë‹¤.\n"

            "ì˜ˆì‹œ) ì§ˆë¬¸ì´ ì»´í“¨í„°ê³µí•™ê³¼ì— ëŒ€í•œ ê²ƒì¼ ê²½ìš°:\n"
            "- \"ë‹¨ì¼ì „ê³µ\" ì‚¬ìš©ìëŠ” ì»´í“¨í„°ê³µí•™ê³¼ë§Œ ì „ê³µ\n"
            "- \"ë‹¤ì „ê³µ(ì»´ê³µ)\" ì‚¬ìš©ìëŠ” ì»´í“¨í„°ê³µí•™ê³¼ê°€ ì œ1ì „ê³µ + ë‹¤ë¥¸ í•™ê³¼ ë³µìˆ˜ì „ê³µ\n"
            "- \"ë‹¤ì „ê³µ(íƒ€ì „ê³µ)\" ì‚¬ìš©ìëŠ” ë‹¤ë¥¸ í•™ê³¼ê°€ ì œ1ì „ê³µ + ì»´í“¨í„°ê³µí•™ê³¼ ë³µìˆ˜ì „ê³µ\n"

            "ì§ˆë¬¸ì´ ì–´ëŠ ì „ê³µ ìœ í˜•ì— í•´ë‹¹í•˜ëŠ”ì§€ ëª…í™•í•˜ì§€ ì•Šë”ë¼ë„, ê° ê²½ìš°ì— ë”°ë¼ ë‹¬ë¼ì§€ëŠ” ë‚´ìš©ì„ **ëª¨ë‘ ë¶„ë¦¬ëœ ë¬¸ë‹¨**ìœ¼ë¡œ ë‚˜ëˆ  ì„¤ëª…í•˜ì„¸ìš”.\n"
            "ì œê³µëœ contextì—ì„œ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´ ì°¾ì„ ìˆ˜ ì—†ë‹¤ê³  ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•´ì£¼ì„¸ìš”.\n"
            
        )

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"context:\n{context}\n\nì§ˆë¬¸: {query}\në‹µë³€:"}
        ]
        model_name = "gpt-4o"
    
    elif cat == 2:
        context = "\n\n".join([
            f"[{meta.get('university', 'Unknown University')} - {meta.get('major', 'Unknown Major')} - {meta.get('source_file', 'Unknown Source')}]\n{doc}"
            for doc, meta in context_docs
        ])

        prompt = (
            "ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµì˜ í•™ì‚¬ ìš”ëŒ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "- ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë”ë¼ë„, ê´€ë ¨ í•™ê³¼ ë˜ëŠ” ê·œì • ë¬¸ì„œë¥¼ ëª¨ë‘ ì°¸ê³ í•˜ì—¬ ê°€ëŠ¥í•œ ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•˜ì„¸ìš”.\n"
            "- ê°™ì€ ê³¼ëª©ì— ëŒ€í•œ ì„¤ëª…ì´ ì—¬ëŸ¬ í•™ê³¼ ë˜ëŠ” ì „ê³µì—ì„œ ë°˜ë³µë  ê²½ìš°, **ëª¨ë“  ê´€ë ¨ ë¬¸ì„œì—ì„œ ë‚˜ì˜¨ ì„¤ëª…ì„ ë¹ ì§ì—†ì´ í¬í•¨**í•˜ì„¸ìš”.\n"
            "- ê°ê°ì˜ ì„¤ëª…ì€ **ì¶œì²˜ í•™ê³¼ëª… ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ì„ ë¶„ë¦¬í•˜ì—¬ ì¶œë ¥**í•˜ê³ , ì¤‘ë³µëœ ë‚´ìš©ì´ ìˆë”ë¼ë„ **í•™ê³¼ ë¬¸ë§¥ ë‚´ì—ì„œëŠ” ìƒëµí•˜ì§€ ë§ê³  ëª¨ë‘ ì¶œë ¥**í•˜ì„¸ìš”.\n"
            "- ìš”ì•½í•˜ì§€ ë§ˆì„¸ìš”. **ëª¨ë“  í•™ê³¼ë³„ ì„¤ëª…ì„ ì „ë¶€ ë‚˜ì—´**í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.\n"
            "- ì œê³µëœ contextì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ê²½ìš°, \"ì œê³µëœ ì •ë³´ì—ì„œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\"ë¼ê³  ì¶œë ¥í•˜ì„¸ìš”.\n"

            
        )

        messages = [
            {"role": "system", "content": prompt},
            {"role": "user", "content": f"context:\n{context}\n\nì§ˆë¬¸: {query}\në‹µë³€:"}
        ]
        #model_name = "ft:gpt-3.5-turbo-0125:capston::Bdtr05OS"
        model_name="gpt-4o"

    else:
        
        context = context_docs
        prompt = (
            "ë‹¹ì‹ ì€ ì„œê°•ëŒ€í•™êµì˜ ê³µì§€ì‚¬í•­ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ë‹µë³€í•˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n"
            #"ë‹¤ìŒ ì˜ˆì‹œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ë˜, ë§í¬ëŠ” í•œ ë²ˆë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n"
            "ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë”ë¼ë„, ì œê³µëœ ê³µì§€ contextë¥¼ ë°”íƒ•ìœ¼ë¡œ ê·œì •ê³¼ ì‚¬ì‹¤ì— ê·¼ê±°í•´ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.\n"
            "ê°€ëŠ¥í•œ í•œ ì§ˆë¬¸ê³¼ í‚¤ì›Œë“œê°€ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê³µì§€ë¥¼ ì°¾ì•„ì„œ ì œì‹œí•˜ì„¸ìš”.\n"
            "ì—¬ëŸ¬ ê°œì˜ ê³µì§€ê°€ ê´€ë ¨ ìˆë‹¤ë©´, ë‚ ì§œ(date)ê°€ ê°€ì¥ ìµœì‹ ì¸ ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ ì¶œë ¥í•˜ì„¸ìš”.\n"
            "ì œê³µëœ contextì— ê´€ë ¨ ì •ë³´ê°€ ì—†ë‹¤ë©´, 'ê´€ë ¨ ê³µì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'ë¼ê³  ë‹µë³€í•˜ì„¸ìš”.\n"
            "ë§í¬ëŠ” ë°˜ë“œì‹œ í•œ ë²ˆë§Œ ì¶œë ¥í•˜ê³ , ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì„ ì‚¬ìš©í•˜ì§€ ë§ê³  ìˆœìˆ˜í•œ URLë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n\n"
        )


        messages = [
            {"role": "system", "content": prompt},
            # ğŸŸ¡ One-shot ì˜ˆì‹œ
            {"role": "user", "content": "context:\n[ì¡¸ì—…] 2023í•™ë…„ë„ í›„ê¸°(2024ë…„ 8ì›”) ì¡¸ì—…_í•™ìœ„ì¦ ë°°ë¶€ ë° í•™ìœ„ê°€ìš´ ëŒ€ì—¬ ì•ˆë‚´|2024.07.30|https://sogang.ac.kr/ko/detail/\n\nì§ˆë¬¸: í•™ìœ„ ê°€ìš´ì€ ì–´ë””ì„œ ëŒ€ì—¬í•  ìˆ˜ ìˆì–´?\në‹µë³€:"},
            {"role": "assistant", "content": "í•™ìœ„ ê°€ìš´ ëŒ€ì—¬ì™€ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒ ê³µì§€ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\nì œëª©:[ì¡¸ì—…] 2023í•™ë…„ë„ í›„ê¸°(2024ë…„ 8ì›”) ì¡¸ì—…_í•™ìœ„ì¦ ë°°ë¶€ ë° í•™ìœ„ê°€ìš´ ëŒ€ì—¬ ì•ˆë‚´\nì—…ë¡œë“œì¼ì: 2024.07.30\në§í¬:https://sogang.ac.kr/ko/detail/\n"},
            {"role": "user", "content": f"context:\n{context}\n\nì§ˆë¬¸: {query}\në‹µë³€:"}
        ]
        model_name = "gpt-4o"

    total_tokens = count_total_tokens(messages, model="gpt-4o")
    max_tokens_model = 128000 # ëª¨ë¸ì˜ ìµœëŒ€ í† í° (gpt-4o ê¸°ì¤€)
    max_response_tokens = 4096 # ë‹µë³€ìœ¼ë¡œ ë°›ê³ ì í•˜ëŠ” ìµœëŒ€ í† í° ìˆ˜

    # í”„ë¡¬í”„íŠ¸ê°€ ë„ˆë¬´ ê¸¸ ê²½ìš°, ëª¨ë¸ì˜ ìµœëŒ€ í† í° ìˆ˜ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ìë¥´ê±°ë‚˜,
    # ë‹µë³€ ìƒì„± í† í° ìˆ˜ë¥¼ ê³ ë ¤í•˜ì—¬ ì…ë ¥ í† í°ì„ ì¡°ì ˆí•´ì•¼ í•¨.
    # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœ ê²½ê³ ë§Œ ì¶œë ¥
    if total_tokens > max_tokens_model - max_response_tokens : # ëª¨ë¸ í•œê³„ - ì‘ë‹µ í† í° = í”„ë¡¬í”„íŠ¸ ìµœëŒ€
        print(f"âš ï¸ ì „ì²´ í”„ë¡¬í”„íŠ¸ í† í° ìˆ˜ê°€ {total_tokens}ê°œë¡œ ëª¨ë¸ ì œí•œ({max_tokens_model})ì„ ì´ˆê³¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¼ë¶€ ì»¨í…ìŠ¤íŠ¸ê°€ ì˜ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        # ì‹¤ì œë¡œëŠ” contextë¥¼ ì¤„ì´ëŠ” ë¡œì§ì´ í•„ìš”í•©ë‹ˆë‹¤.
    try:
        response = openai.ChatCompletion.create(
            model=model_name,
            messages=messages,
            max_tokens=4096, # ë‹µë³€ í† í° ìˆ˜ ì œí•œ
            temperature = 0.3,
            top_p = 0.9
        )
        return response.choices[0].message['content'] # ìˆ˜ì •: .message.content -> .message['content']
    except openai.error.InvalidRequestError as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # í† í° ì´ˆê³¼ ì—ëŸ¬ì˜ ê²½ìš°, ì—¬ê¸°ì„œ contextë¥¼ ì¤„ì—¬ì„œ ì¬ì‹œë„í•˜ëŠ” ë¡œì§ì„ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        if "maximum context length" in str(e):
            return "ì§ˆë¬¸ê³¼ ì°¸ê³  ë¬¸ì„œì˜ ë‚´ìš©ì´ ë„ˆë¬´ ê¸¸ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë” ì§§ê²Œ ì§ˆë¬¸í•´ì£¼ì‹œê±°ë‚˜, í•„í„°ë§ì„ í†µí•´ ë¬¸ì„œ ë²”ìœ„ë¥¼ ì¤„ì—¬ì£¼ì„¸ìš”."
        return "ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
    except Exception as e: # ë‹¤ë¥¸ ì˜ˆì™¸ ì²˜ë¦¬
        print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ë¡œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


#query preprocess module
KOREAN_PARTICLE_PATTERN = r'(ì€|ëŠ”|ì´|ê°€|ì„|ë¥¼|ì—|ì—ì„œ|ìœ¼ë¡œ|ë¡œ|ë„|ë§Œ|ê¹Œì§€|ë¶€í„°|ì¡°ì°¨|ì¸ë°|ê³ |ì™€|ê³¼|ë§ˆì €|ì²˜ëŸ¼|ê»˜ì„œ|ë°–ì—|ì´ë©°|ì´ê³ |ì´ë‚˜|ë¼ë„|ë¼ê³ |ë¼ëŠ”|ë“ ì§€|ë§Œí¼|ì•¼)?'

def preprocess_query(query):
    used_majors = []
    replaced_ranges = []

    for full_name, variants in ABBREVIATION_GROUPS.items():
        for variant in sorted(variants, key=lambda x: -len(x)):
            # ë³€í˜•ì–´ê°€ ì¡°ì‚¬ì™€ í•¨ê»˜ ë¶™ì€ í˜•íƒœë¡œ ëë‚  ë•Œë„ ë§¤ì¹­
            pattern = re.compile(rf'(?<!\w)({re.escape(variant)}){KOREAN_PARTICLE_PATTERN}')

            def replacer(m):
                start, end = m.start(1), m.end(1)  # variantë§Œí¼ì˜ ë²”ìœ„ë¡œ ë¹„êµ
                for r_start, r_end in replaced_ranges:
                    if not (end <= r_start or start >= r_end):
                        return m.group(0)  # ì´ë¯¸ ê²¹ì¹œ ê²½ìš° ì¹˜í™˜í•˜ì§€ ì•ŠìŒ

                replaced_ranges.append((start, start + len(full_name)))
                if full_name not in used_majors:
                    used_majors.append(full_name)
                particle = m.group(2) or ""
                return full_name + particle

            query = pattern.sub(replacer, query)

    return query

    
def extract_date_key_from_query(query: str) -> str | None:
    """
    queryì— DATE_GROUPSì˜ value ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ë©´ í•´ë‹¹ keyë¥¼ ë°˜í™˜.
    ì—†ìœ¼ë©´ None ë°˜í™˜.
    """
    for key, phrases in DATE_GROUPS.items():
        for phrase in phrases:
            if phrase in query:
                return key
    return None

# âœ… ë©”ì¸ ì‹¤í–‰ ë£¨í”„
if __name__ == "__main__":
    print("ğŸ’¬ í•™ì‚¬ìš”ëŒ ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì‹œì‘ë¨.")

    collection_data = load_corpus_by_collection()
    if not collection_data:
        print("âš ï¸ ë¡œë“œëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. DBë¥¼ ë¨¼ì € ìƒì„±í•˜ê±°ë‚˜ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        exit()

    # âœ… ì»¬ë ‰ì…˜ë³„ retriever ì´ˆê¸°í™”
    retrievers = {}
    for col_name, content in collection_data.items():
        retrievers[col_name] = HybridRetriever(
            content["documents"],
            content["metadatas"],
            collection_name=col_name
        )

    # âœ… major ëª©ë¡ë„ í•¨ê»˜ ì €ì¥
    majors_by_collection = {
        col_name: content["majors"]
        for col_name, content in collection_data.items()
        if "majors" in content
    }   

    # âœ… ì¹´í…Œê³ ë¦¬ â†’ ì»¬ë ‰ì…˜ ì´ë¦„ ë§¤í•‘
    category_to_collection = {
        "1": "collection_course",
        "2": "collection_subjectinfo_chunk",
        "3": "collection_notice"
    }

    while True:
        #category ì´ˆê¸°í™”
        print("\nì–´ë–¤ ì¹´í…Œê³ ë¦¬ì˜ ì§ˆë¬¸ì„ í• ì§€ ê³¨ë¼ì£¼ì„¸ìš”.")
        cat = input("\n1. ê³¼ëª©/ì „ê³µ ì´ìˆ˜ ìš”ê±´ 2. ê³¼ëª© ì •ë³´ 3. í•™ì‚¬ ê³µì§€\n-> ")

        if cat not in category_to_collection:
            print("âš ï¸ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. 1, 2 ë˜ëŠ” 3ì„ ì…ë ¥í•˜ì„¸ìš”.")
            continue
        else:
            break
    
    selected_collection = category_to_collection[cat]

    if selected_collection not in retrievers:
        print(f"âŒ ì„ íƒí•œ ì»¬ë ‰ì…˜ '{selected_collection}'ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        exit()

    retriever = retrievers[selected_collection]

    if selected_collection != "collection_notice":
        unique_majors = majors_by_collection[selected_collection]


    while True:
        query = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œë¥¼ ì›í•˜ë©´ exitì„, category ë³€ê²½ì„ ì›í•˜ë©´ catì„ ì…ë ¥í•´ì£¼ì„¸ìš”.): ")
        if query.lower().strip() == "exit":
            print("ğŸš«ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n")
            break
        elif query.lower().strip() == "cat":
            print("\nì–´ë–¤ ì¹´í…Œê³ ë¦¬ì˜ ì§ˆë¬¸ì„ í• ì§€ ê³¨ë¼ì£¼ì„¸ìš”.")
            cat = input("\n1. ê³¼ëª©/ì „ê³µ ì´ìˆ˜ ìš”ê±´ 2. ê³¼ëª© ì •ë³´ 3. í•™ì‚¬ ê³µì§€\n-> ")

            if cat not in category_to_collection:
                print("âš ï¸ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. 1, 2 ë˜ëŠ” 3ì„ ì…ë ¥í•˜ì„¸ìš”.")
                continue
    
            selected_collection = category_to_collection[cat]

            if selected_collection not in retrievers:
                print(f"âŒ ì„ íƒí•œ ì»¬ë ‰ì…˜ '{selected_collection}'ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                break

            retriever = retrievers[selected_collection]
            if selected_collection != "collection_notice":
                unique_majors = majors_by_collection[selected_collection]

            query = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œë¥¼ ì›í•˜ë©´ exitì„, category ë³€ê²½ì„ ì›í•˜ë©´ catì„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.): ")

        if selected_collection == "collection_notice":
            #ì‹œê¸° ì¶”ì¶œ
            date_keyword = extract_date_key_from_query(query)
            

             # ê¸°ë³¸ê°’ ì„¤ì •
            if date_keyword is None:
                print("â„¹ï¸ íŠ¹ì • ì‹œê¸° í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. '2025-1' ë¬¸ì„œë§Œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
                date_keyword = "2025-1"
            else:
                print(f"âœ¨ '{date_keyword}' ê´€ë ¨ ì •ë³´ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")

            query = query.strip()+" "+date_keyword+"ì™€ ê´€ë ¨ëœ ê³µì§€ë¥¼ ì°¾ì•„ì¤˜."
            #print(query)

            # âœ… í•´ë‹¹ ì‹œê¸°ì˜ ë¬¸ì„œë§Œ í•„í„°ë§
            all_docs = collection_data[selected_collection]["documents"]
            all_metas = collection_data[selected_collection]["metadatas"]

            answer=generate_answer(query, all_docs, cat=3)

        elif selected_collection == "collection_subjectinfo_chunk":
            # 1) ì¶•ì•½ì–´ ê·¸ë£¹ ì¹˜í™˜ ì ìš©
            query = preprocess_query(query)

            # 2) ë³€í™˜ëœ ì§ˆì˜ë¡œ í•™ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
            major_filter_keyword = extract_major_keyword(query, unique_majors,threshold = 80)

            if major_filter_keyword:
                print(f"âœ¨ '{major_filter_keyword}' ê´€ë ¨ ì •ë³´ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            else:
                print("â„¹ï¸ íŠ¹ì • í•™ê³¼ í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
                sub = extract_subject_by_rapidfuzz(query)
                query = query.strip()+" "+sub[0]+" ê³¼ëª©"

            print(f"query: {query}")
        

            # 3) í•„í„°ë§ í‚¤ì›Œë“œë¥¼ retrieverì— ì „ë‹¬
            top_docs_with_meta = retriever.retrieve(query, top_k_bm25=10, top_k_dpr=3, filter_major=major_filter_keyword,alpha=0.8,cat= 2)
            print(cat)

            if not top_docs_with_meta:
                print("\nğŸ§  chatbot ì‘ë‹µ:\nê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                continue # ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ

            answer = generate_answer(query, top_docs_with_meta, cat=2)

            print("\nğŸ“ ì°¸ê³ í•œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:")
            for doc_content, meta in top_docs_with_meta: # ë¬¸ì„œ ë‚´ìš©ë„ í•¨ê»˜ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                # print(f" - (ë‚´ìš© ì¼ë¶€: {doc_content[:50]}...) ë©”íƒ€ë°ì´í„°: {meta}")
                print(f" - ë©”íƒ€ë°ì´í„°: {meta}")
        else:
            # 1) ì¶•ì•½ì–´ ê·¸ë£¹ ì¹˜í™˜ ì ìš©
            query = preprocess_query(query)

            print(f"query: {query}")

            # 2) ë³€í™˜ëœ ì§ˆì˜ë¡œ í•™ê³¼ í‚¤ì›Œë“œ ì¶”ì¶œ
            major_filter_keyword = extract_major_keyword(query, unique_majors,threshold = 60)

            if major_filter_keyword:
                print(f"âœ¨ '{major_filter_keyword}' ê´€ë ¨ ì •ë³´ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
            else:
                print("â„¹ï¸ íŠ¹ì • í•™ê³¼ í‚¤ì›Œë“œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì „ì²´ ë¬¸ì„œì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤.")
        

            # 3) í•„í„°ë§ í‚¤ì›Œë“œë¥¼ retrieverì— ì „ë‹¬
            top_docs_with_meta = retriever.retrieve(query, top_k_bm25=10, top_k_dpr=3, filter_major=major_filter_keyword,cat=1)

            if not top_docs_with_meta:
                print("\nğŸ§  chatbot ì‘ë‹µ:\nê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ í•´ì£¼ì‹œê±°ë‚˜ í‚¤ì›Œë“œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
                continue # ë‹¤ìŒ ì§ˆë¬¸ìœ¼ë¡œ

            answer = generate_answer(query, top_docs_with_meta, cat=1)

            print("\nğŸ“ ì°¸ê³ í•œ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:")
            for doc_content, meta in top_docs_with_meta: 
                print(f" - ë©”íƒ€ë°ì´í„°: {meta}")

        print(f"\nğŸ§  chatbot ì‘ë‹µ:\n{answer}")